# ä»è£¸æœºæ­å»ºSoperatorå®Œæ•´æµç¨‹

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†ä»ç‰©ç†æœåŠ¡å™¨ï¼ˆè£¸æœºï¼‰å¼€å§‹ï¼Œå®Œæ•´æ­å»ºSoperator Kubernetes HPCé›†ç¾¤çš„å…¨è¿‡ç¨‹ã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬ç¯å¢ƒå‡†å¤‡ã€Kubernetesé›†ç¾¤æ­å»ºã€GPUç¯å¢ƒé…ç½®ã€Soperatoréƒ¨ç½²ã€é›†ç¾¤éªŒè¯å’Œç›‘æ§é…ç½®ç­‰å…­ä¸ªä¸»è¦é˜¶æ®µã€‚

### ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

```mermaid
graph TB
    subgraph "ç‰©ç†å±‚"
        A1[GPUèŠ‚ç‚¹ xN] --> A2[ç½‘ç»œè®¾å¤‡]
        A2 --> A3[å­˜å‚¨æœåŠ¡å™¨]
    end

    subgraph "æ“ä½œç³»ç»Ÿå±‚"
        B1[Ubuntu 24.04] --> B2[Dockerå®¹å™¨è¿è¡Œæ—¶]
        B2 --> B3[Kubernetesé›†ç¾¤]
    end

    subgraph "åº”ç”¨å±‚"
        C1[Soperator] --> C2[Slurmé›†ç¾¤]
        C2 --> C3[HPCå·¥ä½œè´Ÿè½½]
    end

    A1 --> B1
    B1 --> C1
```

## ğŸ–¥ï¸ é˜¶æ®µ1: è£¸æœºç¯å¢ƒå‡†å¤‡

### 1.1 ç¡¬ä»¶è¦æ±‚æ£€æŸ¥

#### åŸºç¡€ç¡¬ä»¶è¦æ±‚
- **CPU**: x86_64æ¶æ„ï¼Œè‡³å°‘4æ ¸
- **å†…å­˜**: è‡³å°‘8GB RAM
- **å­˜å‚¨**: è‡³å°‘100GBå¯ç”¨ç©ºé—´
- **ç½‘ç»œ**: åƒå…†ä»¥å¤ªç½‘

#### GPUç¡¬ä»¶è¦æ±‚ï¼ˆå¯é€‰ï¼‰
- **GPU**: NVIDIA GPUï¼ˆæ”¯æŒCUDA 12.9ï¼‰
- **æ˜¾å­˜**: è‡³å°‘8GB VRAM
- **é©±åŠ¨**: å…¼å®¹NVIDIAé©±åŠ¨550.54.14+

```bash
# æ£€æŸ¥CPUå’Œå†…å­˜ä¿¡æ¯
lscpu
free -h

# æ£€æŸ¥GPUä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
nvidia-smi
lspci | grep -i nvidia

# æ£€æŸ¥ç½‘ç»œæ¥å£
ip addr show
```

### 1.2 æ“ä½œç³»ç»Ÿå®‰è£…

#### å®‰è£…Ubuntu 24.04 Server
1. ä¸‹è½½Ubuntu 24.04 Server ISOé•œåƒ
2. åˆ›å»ºå¯å¯åŠ¨USBè®¾å¤‡
3. æ‰§è¡Œæœ€å°åŒ–å®‰è£…ï¼Œä»…åŒ…å«SSH serverç»„ä»¶
4. é…ç½®æ—¶åŒºå’Œç½‘ç»œè®¾ç½®

#### ç³»ç»Ÿæ›´æ–°å’ŒåŸºç¡€å·¥å…·å®‰è£…
```bash
# æ›´æ–°ç³»ç»Ÿè½¯ä»¶åŒ…
sudo apt update && sudo apt upgrade -y

# å®‰è£…åŸºç¡€å·¥å…·
sudo apt install -y curl wget git htop iotop vim net-tools \
    software-properties-common apt-transport-https ca-certificates \
    gnupg lsb-release
```

### 1.3 ç½‘ç»œé…ç½®

#### é…ç½®é™æ€IPåœ°å€
```bash
# ç¼–è¾‘ç½‘ç»œé…ç½®æ–‡ä»¶
sudo netplan edit

# ç¤ºä¾‹é…ç½®
network:
  version: 2
  ethernets:
    ens192:
      dhcp4: no
      addresses: [192.168.1.100/24]
      gateway4: 192.168.1.1
      nameservers:
        addresses: [8.8.8.8, 8.8.4.4]

# åº”ç”¨é…ç½®
sudo netplan apply
```

#### é…ç½®hostsæ–‡ä»¶
```bash
sudo tee /etc/hosts << EOF
127.0.0.1 localhost
<master-ip> k8s-master
<worker1-ip> k8s-worker1
<worker2-ip> k8s-worker2
EOF
```

### 1.4 Kubernetesç³»ç»Ÿé…ç½®

#### ç¦ç”¨Swapï¼ˆKubernetesè¦æ±‚ï¼‰
```bash
# ç«‹å³ç¦ç”¨swap
sudo swapoff -a

# æ°¸ä¹…ç¦ç”¨swap
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
```

#### é…ç½®å†…æ ¸æ¨¡å—
```bash
# åŠ è½½å¿…è¦çš„å†…æ ¸æ¨¡å—
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter
```

#### é…ç½®å†…æ ¸å‚æ•°
```bash
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
net.ipv4.conf.all.forwarding        = 1
EOF

# ç«‹å³åº”ç”¨å†…æ ¸å‚æ•°
sudo sysctl --system
```

## ğŸ”§ é˜¶æ®µ2: Kubernetesé›†ç¾¤æ­å»º

### 2.1 å®‰è£…å®¹å™¨è¿è¡Œæ—¶ï¼ˆcontainerdï¼‰

#### å®‰è£…containerd
```bash
# æ·»åŠ Docker GPGå¯†é’¥
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

# ï¿½ï¿½ï¿½åŠ Dockerä»“åº“
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# å®‰è£…containerd
sudo apt update
sudo apt install -y containerd.io
```

#### é…ç½®containerd
```bash
# åˆ›å»ºé…ç½®ç›®å½•
sudo mkdir -p /etc/containerd

# ç”Ÿæˆé»˜è®¤é…ç½®
sudo containerd config default | sudo tee /etc/containerd/config.toml

# ä¿®æ”¹é…ç½®ä½¿ç”¨systemd cgroup
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml

# å¯åŠ¨å¹¶å¯ç”¨containerdæœåŠ¡
sudo systemctl restart containerd
sudo systemctl enable containerd
```

### 2.2 å®‰è£…Kubernetesç»„ä»¶

#### æ·»åŠ Kuberneteså®˜æ–¹ä»“åº“
```bash
# æ·»åŠ Kubernetes GPGå¯†é’¥
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# æ·»åŠ Kubernetesä»“åº“
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

# æ›´æ–°åŒ…åˆ—è¡¨
sudo apt update
```

#### å®‰è£…kubeletã€kubeadmå’Œkubectl
```bash
# å®‰è£…Kubernetesç»„ä»¶
sudo apt install -y kubelet kubeadm kubectl

# é”å®šç‰ˆæœ¬ï¼Œé˜²æ­¢è‡ªåŠ¨æ›´æ–°
sudo apt-mark hold kubelet kubeadm kubectl

# å¯åŠ¨å¹¶å¯ç”¨kubeletæœåŠ¡
sudo systemctl enable kubelet
```

### 2.3 Kubernetesé›†ç¾¤åˆå§‹åŒ–

#### MasterèŠ‚ç‚¹åˆå§‹åŒ–
```bash
# åœ¨MasterèŠ‚ç‚¹ä¸Šæ‰§è¡Œé›†ç¾¤åˆå§‹åŒ–
sudo kubeadm init \
  --pod-network-cidr=10.244.0.0/16 \
  --apiserver-advertise-address=<master-ip> \
  --ignore-preflight-errors=all

# é…ç½®kubectlè®¿é—®æƒé™
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# ä¿å­˜workerèŠ‚ç‚¹joinå‘½ä»¤
kubeadm token create --print-join-command > /tmp/kubeadm-join.sh
```

#### WorkerèŠ‚ç‚¹åŠ å…¥é›†ç¾¤
```bash
# åœ¨æ¯ä¸ªWorkerèŠ‚ç‚¹æ‰§è¡Œjoinå‘½ä»¤
sudo kubeadm join <master-ip>:6443 \
  --token <token> \
  --discovery-token-ca-cert-hash <hash>

# æˆ–ä½¿ç”¨ä¿å­˜çš„è„šæœ¬
sudo bash /tmp/kubeadm-join.sh
```

#### éªŒè¯é›†ç¾¤çŠ¶æ€
```bash
# æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes -o wide

# æ£€æŸ¥ç³»ç»ŸPodçŠ¶æ€
kubectl get pods -n kube-system
```

### 2.4 å®‰è£…CNIç½‘ç»œæ’ä»¶

#### å®‰è£…Ciliumï¼ˆæ¨èï¼‰
```bash
# ä¸‹è½½Cilium CLI
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/master/stable.txt)
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-amd64.tar.gz{,.sha256sum}

# éªŒè¯ä¸‹è½½æ–‡ä»¶
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum

# è§£å‹å¹¶å®‰è£…
sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
rm cilium-linux-amd64.tar.gz{,.sha256sum}

# å®‰è£…Ciliumï¼ˆå¯ç”¨kube-proxyæ›¿æ¢æ¨¡å¼ï¼‰
cilium install --set kubeProxyReplacement=strict

# éªŒè¯Ciliumå®‰è£…
cilium status
kubectl get pods -n kube-system | grep cilium
```

#### æµ‹è¯•ç½‘ç»œè¿é€šæ€§
```bash
# å®‰è£…ç½‘ç»œè¿é€šæ€§æµ‹è¯•å·¥å…·
cilium connectivity test

# æ£€æŸ¥Ciliumé…ç½®
cilium config view
```

### 2.5 é…ç½®å…±äº«å­˜å‚¨

#### å®‰è£…NFSæœåŠ¡å™¨
```bash
# åœ¨MasterèŠ‚ç‚¹å®‰è£…NFSæœåŠ¡å™¨
sudo apt install -y nfs-kernel-server

# åˆ›å»ºå…±äº«ç›®å½•
sudo mkdir -p /export/slurm
sudo chown nobody:nogroup /export/slurm
sudo chmod 777 /export/slurm

# é…ç½®NFSå¯¼å‡º
echo '/export/slurm *(rw,fsid=0,async,no_subtree_check,no_auth_nlm,insecure,no_root_squash)' | sudo tee -a /etc/exports

# æ›´æ–°NFSå¯¼å‡ºå¹¶é‡å¯æœåŠ¡
sudo exportfs -a
sudo systemctl restart nfs-kernel-server
sudo systemctl enable nfs-kernel-server

# éªŒè¯NFSæœåŠ¡
sudo exportfs -v
```

#### å®‰è£…NFS CSIé©±åŠ¨
```bash
# æ·»åŠ NFS CSI Helmä»“åº“
helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts
helm repo update

# å®‰è£…NFS CSIé©±åŠ¨
helm install csi-driver-nfs csi-driver-nfs/csi-driver-nfs --namespace kube-system

# éªŒè¯CSIé©±åŠ¨å®‰è£…
kubectl get pods -n kube-system | grep nfs
```

#### åˆ›å»ºStorageClass
```bash
# åˆ›å»ºNFS StorageClass
kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-csi
provisioner: nfs.csi.k8s.io
parameters:
  server: $(hostname -I | awk '{print $1}')
  share: /export/slurm
reclaimPolicy: Retain
volumeBindingMode: Immediate
EOF

# éªŒè¯StorageClassåˆ›å»º
kubectl get storageclass
```

## ğŸš€ é˜¶æ®µ3: GPUç¯å¢ƒé…ç½®ï¼ˆå¯é€‰ï¼‰

### 3.1 å®‰è£…NVIDIAé©±åŠ¨

#### åœ¨GPUèŠ‚ç‚¹å®‰è£…é©±åŠ¨
```bash
# å®‰è£…å¿…è¦çš„ç¼–è¯‘å·¥å…·
sudo apt install -y build-essential dkms

# ä¸‹è½½CUDAå®‰è£…åŒ…ï¼ˆåŒ…å«é©±åŠ¨ï¼‰
wget https://developer.download.nvidia.com/compute/cuda/12.9.0/local_installers/cuda_12.9.0_550.54.14_linux.run

# å®‰è£…é©±åŠ¨ï¼ˆä»…å®‰è£…é©±åŠ¨ï¼Œä¸å®‰è£…CUDAå·¥å…·åŒ…ï¼‰
sudo sh cuda_12.9.0_550.54.14_linux.run --silent --driver

# é‡å¯ç³»ç»Ÿä»¥åŠ è½½é©±åŠ¨
sudo reboot
```

#### éªŒè¯é©±åŠ¨å®‰è£…
```bash
# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# æ£€æŸ¥GPUè®¾å¤‡
lspci | grep -i nvidia

# æŸ¥çœ‹é©±åŠ¨æ¨¡å—
lsmod | grep nvidia
```

### 3.2 å®‰è£…NVIDIA GPU Operator

#### æ·»åŠ NVIDIA Helmä»“åº“
```bash
# æ·»åŠ NVIDIA Helmä»“åº“
helm repo add nvidia https://nvidia.github.io/gpu-operator
helm repo update
```

#### å®‰è£…GPU Operator
```bash
# åˆ›å»ºGPUå‘½åç©ºé—´
kubectl create namespace gpu-operator

# å®‰è£…GPU Operator
helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator \
  --set driver.enabled=false \
  --set toolkit.enabled=true \
  --set devicePlugin.enabled=true \
  --set migManager.enabled=false \
  --set mig.strategy=single

# éªŒè¯GPU Operatorå®‰è£…
kubectl get pods -n gpu-operator
kubectl get nodes -o wide
```

#### éªŒè¯GPUèŠ‚ç‚¹
```bash
# æ£€æŸ¥èŠ‚ç‚¹GPUæ ‡ç­¾
kubectl describe nodes | grep -i gpu

# æ£€æŸ¥GPUè®¾å¤‡æ’ä»¶
kubectl logs -n gpu-operator -l app.kubernetes.io/component=nvidia-device-plugin-daemonset
```

## ğŸ“¦ é˜¶æ®µ4: Soperatoréƒ¨ç½²

### 4.1 å®‰è£…å‰ç½®ä¾èµ–

#### å®‰è£…OpenKruise Operator
```bash
# æ·»åŠ OpenKruise Helmä»“åº“
helm repo add openkruise https://openkruise.github.io/charts
helm repo update

# å®‰è£…OpenKruise Operator
helm install kruise openkruise/kruise --namespace kruise-system --create-namespace

# éªŒè¯OpenKruiseå®‰è£…
kubectl get pods -n kruise-system
kubectl get crd | grep kruise
```

#### å®‰è£…å¯é€‰ä¾èµ–ç»„ä»¶

##### å®‰è£…Prometheus CRDsï¼ˆç”¨äºç›‘æ§ï¼‰
```bash
# å®‰è£…Prometheus Operator CRDs
kubectl apply -f https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.68.0/bundle.yaml

# éªŒè¯Prometheus CRDs
kubectl get crd | grep monitoring
```

##### å®‰è£…AppArmor CRDsï¼ˆç”¨äºå®‰å…¨ï¼‰
```bash
# å®‰è£…AppArmor Operator
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/apparmor-operator/main/config/crd/bases/apparmorsecurityprofiles.kubernetes.io_profiles.yaml

# éªŒè¯AppArmor CRDs
kubectl get crd | grep apparmor
```

### 4.2 éƒ¨ç½²Soperator Operator

#### åˆ›å»ºSoperatorå‘½åç©ºé—´
```bash
# åˆ›å»ºSoperatorç³»ç»Ÿå‘½åç©ºé—´
kubectl create namespace soperator-system
```

#### æ·»åŠ Soperator Helmä»“åº“
```bash
# æ·»åŠ Soperatorå®˜æ–¹ä»“åº“
helm repo add soperator https://nebius.github.io/soperator
helm repo update

# æŸ¥çœ‹å¯ç”¨ç‰ˆæœ¬
helm search repo soperator
```

#### éƒ¨ç½²Soperator
```bash
# éƒ¨ç½²ç¨³å®šç‰ˆæœ¬
helm install soperator soperator/soperator \
  --namespace soperator-system \
  --set logLevel=info \
  --set enableWebhooks=true \
  --set enableTopologyController=true

# éªŒè¯Soperatorå®‰è£…
kubectl get pods -n soperator-system
kubectl get crd | grep slurm

# æŸ¥çœ‹Soperatoræ—¥å¿—
kubectl logs -n soperator-system -l app.kubernetes.io/name=soperator
```

#### æ£€æŸ¥SoperatorçŠ¶æ€
```bash
# æ£€æŸ¥Controller ManagerçŠ¶æ€
kubectl get deployment soperator-controller-manager -n soperator-system

# æ£€æŸ¥WebhookæœåŠ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
kubectl get mutatingwebhookconfigurations | grep soperator
kubectl get validatingwebhookconfigurations | grep soperator
```

### 4.3 åˆ›å»ºSlurmé›†ç¾¤é…ç½®

#### åˆ›å»ºCPUé›†ç¾¤
```bash
# åˆ›å»ºCPUé›†ç¾¤å‘½åç©ºé—´
kubectl create namespace cpu-cluster

# éƒ¨ç½²CPU Slurmé›†ç¾¤
helm install my-cpu-cluster soperator/slurm-cluster \
  --namespace cpu-cluster \
  --set clusterName=my-cpu-cluster \
  --set clusterType=cpu \
  --set slurmNodes.worker.size=2 \
  --set slurmNodes.controller.size=1 \
  --set slurmNodes.login.size=1 \
  --set volumeSources[0].name=jail \
  --set volumeSources[0].createPVC=true \
  --set volumeSources[0].storageClassName=nfs-csi \
  --set volumeSources[0].size=20Gi

# æ£€æŸ¥CPUé›†ç¾¤çŠ¶æ€
kubectl get slurmclusters -n cpu-cluster
kubectl describe slurmcluster my-cpu-cluster -n cpu-cluster
```

#### åˆ›å»ºGPUé›†ç¾¤
```bash
# åˆ›å»ºGPUé›†ç¾¤å‘½åç©ºé—´
kubectl create namespace gpu-cluster

# éƒ¨ç½²GPU Slurmé›†ç¾¤
helm install my-gpu-cluster soperator/slurm-cluster \
  --namespace gpu-cluster \
  --set clusterName=my-gpu-cluster \
  --set clusterType=gpu \
  --set slurmNodes.worker.size=4 \
  --set slurmNodes.controller.size=1 \
  --set slurmNodes.login.size=1 \
  --set volumeSources[0].name=jail \
  --set volumeSources[0].createPVC=true \
  --set volumeSources[0].storageClassName=nfs-csi \
  --set volumeSources[0].size=50Gi

# æ£€æŸ¥GPUé›†ç¾¤çŠ¶æ€
kubectl get slurmclusters -n gpu-cluster
kubectl describe slurmcluster my-gpu-cluster -n gpu-cluster
```

### 4.4 é…ç½®èŠ‚ç‚¹è¿‡æ»¤å™¨

**é‡è¦è¯´æ˜**: èŠ‚ç‚¹è¿‡æ»¤å™¨**åªéœ€è¦é…ç½®ä¸€æ¬¡**ï¼Œåç»­åŠ å…¥çš„æ–°èŠ‚ç‚¹ä¼šè‡ªåŠ¨è¢«è¯†åˆ«å’Œä½¿ç”¨ï¼Œæ— éœ€æ‰‹åŠ¨å¹²é¢„ã€‚

#### èŠ‚ç‚¹è¿‡æ»¤å™¨å·¥ä½œåŸç†

èŠ‚ç‚¹è¿‡æ»¤å™¨é€šè¿‡**æ ‡ç­¾é€‰æ‹©å™¨**å’Œ**äº²å’Œæ€§è§„åˆ™**è‡ªåŠ¨å·¥ä½œï¼Œå®ç°"é›¶é…ç½®"èŠ‚ç‚¹ç®¡ç†ï¼š

```mermaid
graph TB
    subgraph "èŠ‚ç‚¹è‡ªåŠ¨å‘ç°æµç¨‹"
        A[æ–°èŠ‚ç‚¹åŠ å…¥K8s] --> B{èŠ‚ç‚¹æœ‰GPU?}
        B -->|æ˜¯| C[è‡ªåŠ¨æ·»åŠ GPUæ ‡ç­¾]
        B -->|å¦| D[ä¿æŒCPUæ ‡ç­¾]
        C --> E[è¢«GPU-nodesè¿‡æ»¤å™¨è¯†åˆ«]
        D --> F[è¢«CPU-nodesè¿‡æ»¤å™¨è¯†åˆ«]
        E --> G[å¯è°ƒåº¦GPUå·¥ä½œè´Ÿè½½]
        F --> H[å¯è°ƒåº¦CPUå·¥ä½œè´Ÿè½½]
    end
```

#### æ–¹å¼A: ä½¿ç”¨GPU Operatorè‡ªåŠ¨æ ‡ç­¾ï¼ˆæ¨èï¼‰

GPU Operatorå®‰è£…åä¼šè‡ªåŠ¨ä¸ºGPUèŠ‚ç‚¹æ·»åŠ æ ‡ç­¾ï¼š
```bash
# æ£€æŸ¥GPU Operatorè‡ªåŠ¨æ·»åŠ çš„æ ‡ç­¾
kubectl get nodes --show-labels | grep gpu
# é¢„æœŸè¾“å‡º:
# node-gpu-1   accelerator=nvidia-tesla-v100,nvidia.com/gpu.present=true
# node-gpu-2   accelerator=nvidia-tesla-v100,nvidia.com/gpu.present=true
```

#### æ–¹å¼B: æ‰‹åŠ¨æ·»åŠ èŠ‚ç‚¹æ ‡ç­¾ï¼ˆä¸€æ¬¡æ€§æ“ä½œï¼‰

å¦‚æœGPU Operatoræ²¡æœ‰è‡ªåŠ¨æ·»åŠ æ ‡ç­¾ï¼Œå¯ä»¥æ‰‹åŠ¨æ·»åŠ ï¼š
```bash
# ä¸ºGPUèŠ‚ç‚¹æ·»åŠ æ ‡ç­¾ï¼ˆä¸€æ¬¡æ€§æ“ä½œï¼‰
kubectl label node <gpu-node-1> accelerator=nvidia-gpu node-type=gpu
kubectl label node <gpu-node-2> accelerator=nvidia-gpu node-type=gpu

# ä¸ºCPUèŠ‚ç‚¹æ·»åŠ æ ‡ç­¾
kubectl label node <cpu-node-1> node-type=cpu
kubectl label node <cpu-node-2> node-type=cpu
```

#### ä¸ºGPUé›†ç¾¤é…ç½®èŠ‚ç‚¹äº²å’Œæ€§

```bash
# ç¼–è¾‘GPUé›†ç¾¤é…ç½®
kubectl edit slurmcluster my-gpu-cluster -n gpu-cluster
```

åœ¨é…ç½®ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š
```yaml
spec:
  k8sNodeFilters:
    - name: gpu-compute-nodes
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              # è§„åˆ™1: åŸºäºNVIDIAé©±åŠ¨æ ‡ç­¾ï¼ˆæ¨èï¼‰
              - matchExpressions:
                  - key: "nvidia.com/gpu.present"
                    operator: In
                    values: ["true"]
              # è§„åˆ™2: åŸºäºäº‘æä¾›å•†æ ‡ç­¾
              - matchExpressions:
                  - key: "node.kubernetes.io/instance-family"
                    operator: In
                    values: ["p3", "p4", "g4", "g5"]
              # è§„åˆ™3: é€šç”¨GPUæ£€æµ‹
              - matchExpressions:
                  - key: "accelerator"
                    operator: Exists
  slurmNodes:
    worker:
      k8sNodeFilter: gpu-compute-nodes
      size: 4
      resources:
        requests:
          nvidia.com/gpu: "1"
        limits:
          nvidia.com/gpu: "1"
    controller:
      k8sNodeFilter: gpu-compute-nodes
      size: 1
    login:
      k8sNodeFilter: gpu-compute-nodes
      size: 1
```

#### ä¸ºCPUé›†ç¾¤é…ç½®èŠ‚ç‚¹äº²å’Œæ€§

```bash
# ç¼–è¾‘CPUé›†ç¾¤é…ç½®
kubectl edit slurmcluster my-cpu-cluster -n cpu-cluster
```

åœ¨é…ç½®ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š
```yaml
spec:
  k8sNodeFilters:
    - name: cpu-compute-nodes
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              # è§„åˆ™1: æ’é™¤GPUèŠ‚ç‚¹
              - matchExpressions:
                  - key: "nvidia.com/gpu.present"
                    operator: DoesNotExist
              # è§„åˆ™2: æ’é™¤GPUå®ä¾‹ç±»å‹
              - matchExpressions:
                  - key: "node.kubernetes.io/instance-family"
                    operator: NotIn
                    values: ["p3", "p4", "g4", "g5"]
              # è§„åˆ™3: æ’é™¤æ‰€æœ‰GPUç›¸å…³æ ‡ç­¾
              - matchExpressions:
                  - key: "accelerator"
                    operator: DoesNotExist
  slurmNodes:
    worker:
      k8sNodeFilter: cpu-compute-nodes
      size: 2
      resources:
        requests:
          cpu: "2"
          memory: "4Gi"
        limits:
          cpu: "4"
          memory: "8Gi"
    controller:
      k8sNodeFilter: cpu-compute-nodes
      size: 1
    login:
      k8sNodeFilter: cpu-compute-nodes
      size: 1
```

#### éªŒè¯èŠ‚ç‚¹è¿‡æ»¤å™¨é…ç½®

```bash
# 1. æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹å’Œæ ‡ç­¾
kubectl get nodes --show-labels

# 2. æ£€æŸ¥å“ªäº›èŠ‚ç‚¹è¢«GPUè¿‡æ»¤å™¨è¯†åˆ«
kubectl get nodes -l nvidia.com/gpu.present=true

# 3. æ£€æŸ¥å“ªäº›èŠ‚ç‚¹è¢«CPUè¿‡æ»¤å™¨è¯†åˆ«
kubectl get nodes -l '!nvidia.com/gpu.present'

# 4. æŸ¥çœ‹Slurmé›†ç¾¤å®é™…ä½¿ç”¨çš„èŠ‚ç‚¹
kubectl get pods -n gpu-cluster -o wide
kubectl get pods -n cpu-cluster -o wide
```

#### åŠ¨æ€æ‰©å®¹ç¤ºä¾‹

æ·»åŠ æ–°èŠ‚ç‚¹åï¼Œæ— éœ€ä¿®æ”¹èŠ‚ç‚¹è¿‡æ»¤å™¨é…ç½®ï¼š

```bash
# 1. æ–°GPUèŠ‚ç‚¹è‡ªåŠ¨åŠ å…¥åï¼ŒGPU Operatorä¼šè‡ªåŠ¨æ·»åŠ æ ‡ç­¾
# 2. éªŒè¯æ–°èŠ‚ç‚¹è¢«è¯†åˆ«
kubectl get nodes --show-labels | grep new-gpu-node

# 3. ç›´æ¥æ‰©å®¹Slurmé›†ç¾¤ï¼Œæ— éœ€ä¿®æ”¹è¿‡æ»¤å™¨
kubectl patch slurmcluster my-gpu-cluster -n gpu-cluster \
  --type='merge' \
  -p='{"spec":{"slurmNodes":{"worker":{"size":6}}}}'  # ä»4æ‰©åˆ°6

# 4. éªŒè¯æ–°Podè°ƒåº¦åˆ°æ–°èŠ‚ç‚¹
kubectl get pods -n gpu-cluster -o wide --sort-by=.spec.nodeName
```

#### è‡ªåŠ¨åŒ–èŠ‚ç‚¹æ ‡ç­¾è„šæœ¬

åˆ›å»ºè‡ªåŠ¨æ ‡ç­¾è„šæœ¬ `auto-label-nodes.sh`ï¼š
```bash
#!/bin/bash
# auto-label-nodes.sh - è‡ªåŠ¨ä¸ºæ–°èŠ‚ç‚¹æ·»åŠ æ ‡ç­¾

echo "å¼€å§‹è‡ªåŠ¨æ ‡ç­¾èŠ‚ç‚¹..."

# æ£€æµ‹GPUèŠ‚ç‚¹å¹¶æ·»åŠ æ ‡ç­¾
for node in $(kubectl get nodes -o name); do
    node_name=$(echo $node | cut -d'/' -f2)

    # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æœ‰GPUè®¾å¤‡
    if kubectl describe node $node_name | grep -q "nvidia.com/gpu"; then
        echo "å‘ç°GPUèŠ‚ç‚¹: $node_name"
        kubectl label node $node_name node-type=gpu --overwrite
        kubectl label node $node_name nvidia.com/gpu.present=true --overwrite
    else
        echo "å‘ç°CPUèŠ‚ç‚¹: $node_name"
        kubectl label node $node_name node-type=cpu --overwrite
    fi
done

echo "èŠ‚ç‚¹æ ‡ç­¾æ›´æ–°å®Œæˆ"
kubectl get nodes --show-labels
```

#### æœ€ä½³å®è·µå»ºè®®

1. **ä½¿ç”¨GPU Operatorè‡ªåŠ¨æ ‡ç­¾**: æœ€å¯é çš„æ–¹å¼ï¼Œæ— éœ€äººå·¥å¹²é¢„
2. **å¤šè§„åˆ™å¤‡ä»½**: é…ç½®å¤šä¸ªåŒ¹é…è§„åˆ™ï¼Œæé«˜è¯†åˆ«æˆåŠŸç‡
3. **å®šæœŸéªŒè¯**: å®šæœŸæ£€æŸ¥èŠ‚ç‚¹è¿‡æ»¤å™¨æ˜¯å¦æ­£ç¡®è¯†åˆ«æ–°èŠ‚ç‚¹
4. **ç›‘æ§è°ƒåº¦**: ç›‘æ§Podæ˜¯å¦è°ƒåº¦åˆ°æ­£ç¡®çš„èŠ‚ç‚¹ç±»å‹
5. **æ—¥å¿—æ£€æŸ¥**: å®šæœŸæŸ¥çœ‹Soperatoræ—¥å¿—ï¼Œæ’æŸ¥è°ƒåº¦é—®é¢˜

**å…³é”®ä¼˜åŠ¿**: èŠ‚ç‚¹è¿‡æ»¤å™¨é…ç½®ä¸€æ¬¡åï¼Œåç»­æ‰€æœ‰æ–°èŠ‚ç‚¹éƒ½ä¼šè‡ªåŠ¨è¢«æ­£ç¡®åˆ†ç±»å’Œä½¿ç”¨ï¼Œå®ç°çœŸæ­£çš„"é›¶è¿ç»´"èŠ‚ç‚¹ç®¡ç†ã€‚

### 4.5 éªŒè¯PVCåˆ›å»º
```bash
# æ£€æŸ¥PVCçŠ¶æ€
kubectl get pvc -n cpu-cluster
kubectl get pvc -n gpu-cluster

# æ£€æŸ¥å­˜å‚¨ä½¿ç”¨æƒ…å†µ
kubectl describe pvc jail -n gpu-cluster
```

## ğŸ¯ é˜¶æ®µ5: é›†ç¾¤éªŒè¯å’Œæµ‹è¯•

### 5.1 é›†ç¾¤çŠ¶æ€æ£€æŸ¥

#### æ£€æŸ¥Slurmé›†ç¾¤çŠ¶æ€
```bash
# æŸ¥çœ‹æ‰€æœ‰Slurmé›†ç¾¤
kubectl get slurmclusters -A

# æŸ¥çœ‹é›†ç¾¤è¯¦ç»†çŠ¶æ€
kubectl describe slurmcluster my-gpu-cluster -n gpu-cluster
kubectl describe slurmcluster my-cpu-cluster -n cpu-cluster

# æ£€æŸ¥é›†ç¾¤æ¡ä»¶
kubectl get slurmcluster my-gpu-cluster -n gpu-cluster -o jsonpath='{.status.conditions}'
```

#### æ£€æŸ¥PodçŠ¶æ€
```bash
# æŸ¥çœ‹GPUé›†ç¾¤PodçŠ¶æ€
kubectl get pods -n gpu-cluster
kubectl get pods -n gpu-cluster -o wide

# æŸ¥çœ‹CPUé›†ç¾¤PodçŠ¶æ€
kubectl get pods -n cpu-cluster
kubectl get pods -n cpu-cluster -o wide

# æ£€æŸ¥Podé‡å¯æƒ…å†µ
kubectl get pods -n gpu-cluster --sort-by='.status.containerStatuses[0].restartCount'
```

#### æŸ¥çœ‹é›†ç¾¤äº‹ä»¶
```bash
# æŸ¥çœ‹GPUé›†ç¾¤äº‹ä»¶
kubectl get events -n gpu-cluster --sort-by='.lastTimestamp'

# æŸ¥çœ‹CPUé›†ç¾¤äº‹ä»¶
kubectl get events -n cpu-cluster --sort-by='.lastTimestamp'

# æŸ¥çœ‹Soperatoräº‹ä»¶
kubectl get events -n soperator-system --sort-by='.lastTimestamp'
```

### 5.2 ç­‰å¾…é›†ç¾¤å°±ç»ª

#### ç­‰å¾…é›†ç¾¤å˜ä¸ºAvailableçŠ¶æ€
```bash
# ç­‰å¾…GPUé›†ç¾¤å°±ç»ª
kubectl wait --for=condition=Available slurmcluster/my-gpu-cluster -n gpu-cluster --timeout=20m

# ç­‰å¾…CPUé›†ç¾¤å°±ç»ª
kubectl wait --for=condition=Available slurmcluster/my-cpu-cluster -n cpu-cluster --timeout=20m

# æ£€æŸ¥é›†ç¾¤çŠ¶æ€
kubectl get slurmclusters -A -o wide
```

#### éªŒè¯SlurmæœåŠ¡çŠ¶æ€
```bash
# æ£€æŸ¥GPUé›†ç¾¤SlurmçŠ¶æ€
kubectl exec -it login-0 -n gpu-cluster -- sinfo
kubectl exec -it login-0 -n gpu-cluster -- scontrol show daemons

# æ£€æŸ¥CPUé›†ç¾¤SlurmçŠ¶æ€
kubectl exec -it login-0 -n cpu-cluster -- sinfo
kubectl exec -it login-0 -n cpu-cluster -- scontrol show daemons
```

### 5.3 SSHç™»å½•æµ‹è¯•

#### è®¾ç½®SSHç«¯å£è½¬å‘
```bash
# è½¬å‘GPUé›†ç¾¤SSHç«¯å£
kubectl port-forward -n gpu-cluster svc/login 30022:22 &

# è½¬å‘CPUé›†ç¾¤SSHç«¯å£
kubectl port-forward -n cpu-cluster svc/login 30023:22 &
```

#### SSHç™»å½•æµ‹è¯•
```bash
# æµ‹è¯•GPUé›†ç¾¤ç™»å½•
ssh -p 30022 soperator@localhost

# æµ‹è¯•CPUé›†ç¾¤ç™»å½•
ssh -p 30023 soperator@localhost
```

åœ¨SlurmèŠ‚ç‚¹å†…æ‰§è¡Œï¼š
```bash
# æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
sinfo

# æ£€æŸ¥ç”¨æˆ·ç¯å¢ƒ
whoami
pwd
ls -la

# æ£€æŸ¥å…±äº«æ–‡ä»¶ç³»ç»Ÿ
df -h
```

### 5.4 ä½œä¸šæäº¤æµ‹è¯•

#### åˆ›å»ºæµ‹è¯•ä½œä¸šè„šæœ¬
```bash
# åœ¨GPUé›†ç¾¤åˆ›å»ºæµ‹è¯•ä½œä¸š
kubectl exec -it login-0 -n gpu-cluster -- bash -c 'cat > gpu_test_job.sh << EOF
#!/bin/bash
#SBATCH --ntasks=2
#SBATCH --gres=gpu:1
#SBATCH --time=00:05:00
#SBATCH --output=gpu_test_%j.out
#SBATCH --error=gpu_test_%j.err

echo "Job started at: \$(date)"
echo "Running on node: \$(hostname)"
echo "Job ID: \$SLURM_JOB_ID"

# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# è¿è¡Œç®€å•çš„GPUè®¡ç®—
python3 -c "
import torch
if torch.cuda.is_available():
    print(f'CUDA available: {torch.cuda.is_available()}')
    print(f'GPU count: {torch.cuda.device_count()}')
    print(f'Current device: {torch.cuda.current_device()}')
    print(f'Device name: {torch.cuda.get_device_name(0)}')

    # ç®€å•çŸ©é˜µä¹˜æ³•æµ‹è¯•
    a = torch.randn(1000, 1000).cuda()
    b = torch.randn(1000, 1000).cuda()
    c = torch.matmul(a, b)
    print(f'Matrix multiplication completed: {c.shape}')
else:
    print('CUDA not available')
"

echo "Job completed at: \$(date)"
EOF'

# åœ¨CPUé›†ç¾¤åˆ›å»ºæµ‹è¯•ä½œä¸š
kubectl exec -it login-0 -n cpu-cluster -- bash -c 'cat > cpu_test_job.sh << EOF
#!/bin/bash
#SBATCH --ntasks=2
#SBATCH --cpus-per-task=2
#SBATCH --time=00:05:00
#SBATCH --output=cpu_test_%j.out
#SBATCH --error=cpu_test_%j.err

echo "Job started at: \$(date)"
echo "Running on node: \$(hostname)"
echo "Job ID: \$SLURM_JOB_ID"

# è¿è¡Œç®€å•çš„CPUè®¡ç®—
echo "Performing CPU intensive computation..."
for i in {1..10}; do
    echo "Iteration \$i: \$(date)"
    sleep 1
done

# ç®€å•çš„çŸ©é˜µè®¡ç®—ï¼ˆä½¿ç”¨Pythonï¼‰
python3 -c "
import numpy as np
print('NumPy available:', np.__version__)

# ç®€å•çŸ©é˜µä¹˜æ³•æµ‹è¯•
a = np.random.rand(1000, 1000)
b = np.random.rand(1000, 1000)
c = np.dot(a, b)
print(f'NumPy matrix multiplication completed: {c.shape}')

# è®¡ç®—Piå€¼
n = 1000000
x = np.random.rand(n)
y = np.random.rand(n)
inside = (x**2 + y**2) <= 1
pi_estimate = 4 * np.sum(inside) / n
print(f'Pi estimate: {pi_estimate}')
"

echo "Job completed at: \$(date)"
EOF'
```

#### æäº¤æµ‹è¯•ä½œä¸š
```bash
# æäº¤GPUæµ‹è¯•ä½œä¸š
kubectl exec -it login-0 -n gpu-cluster -- sbatch gpu_test_job.sh

# æäº¤CPUæµ‹è¯•ä½œä¸š
kubectl exec -it login-0 -n cpu-cluster -- sbatch cpu_test_job.sh
```

#### ç›‘æ§ä½œä¸šæ‰§è¡Œ
```bash
# ç›‘æ§GPUé›†ç¾¤ä½œä¸š
kubectl exec -it login-0 -n gpu-cluster -- squeue
kubectl exec -it login-0 -n gpu-cluster -- squeue -u soperator

# ç›‘æ§CPUé›†ç¾¤ä½œä¸š
kubectl exec -it login-0 -n cpu-cluster -- squeue
kubectl exec -it login-0 -n cpu-cluster -- squeue -u soperator

# æŸ¥çœ‹ä½œä¸šè¯¦æƒ…
kubectl exec -it login-0 -n gpu-cluster -- scontrol show job <job_id>
kubectl exec -it login-0 -n cpu-cluster -- scontrol show job <job_id>
```

#### æŸ¥çœ‹ä½œä¸šè¾“å‡º
```bash
# æŸ¥çœ‹GPUä½œä¸šè¾“å‡º
kubectl exec -it login-0 -n gpu-cluster -- cat gpu_test_*.out
kubectl exec -it login-0 -n gpu-cluster -- cat gpu_test_*.err

# æŸ¥çœ‹CPUä½œä¸šè¾“å‡º
kubectl exec -it login-0 -n cpu-cluster -- cat cpu_test_*.out
kubectl exec -it login-0 -n cpu-cluster -- cat cpu_test_*.err
```

## ğŸ” é˜¶æ®µ6: ç›‘æ§å’Œè¿ç»´é…ç½®

### 6.1 å®‰è£…å¥åº·æ£€æŸ¥ç»„ä»¶

#### éƒ¨ç½²ActiveChecks
```bash
# ä¸ºGPUé›†ç¾¤å®‰è£…å¥åº·æ£€æŸ¥
helm install my-gpu-cluster-checks soperator/soperator-activechecks \
  --namespace gpu-cluster \
  --set clusterName=my-gpu-cluster \
  --set schedule="0 */6 * * *" \
  --set checkType=slurmJob

# ä¸ºCPUé›†ç¾¤å®‰è£…å¥åº·æ£€æŸ¥
helm install my-cpu-cluster-checks soperator/soperator-activechecks \
  --namespace cpu-cluster \
  --set clusterName=my-cpu-cluster \
  --set schedule="0 */4 * * *" \
  --set checkType=slurmJob
```

#### æŸ¥çœ‹å¥åº·æ£€æŸ¥çŠ¶æ€
```bash
# æ£€æŸ¥ActiveCheckçŠ¶æ€
kubectl get activechecks -n gpu-cluster
kubectl get activechecks -n cpu-cluster

# æŸ¥çœ‹å¥åº·æ£€æŸ¥æ—¥å¿—
kubectl logs -n gpu-cluster -l app.kubernetes.io/name=soperator-activechecks
kubectl logs -n cpu-cluster -l app.kubernetes.io/name=soperator-activechecks
```

### 6.2 é…ç½®Prometheusç›‘æ§

#### é…ç½®æŒ‡æ ‡å¯¼å‡º
```bash
# æŸ¥çœ‹æŒ‡æ ‡å¯¼å‡ºæœåŠ¡
kubectl get svc -n gpu-cluster | grep exporter
kubectl get svc -n cpu-cluster | grep exporter

# è½¬å‘æŒ‡æ ‡ç«¯å£
kubectl port-forward -n gpu-cluster svc/my-gpu-cluster-soperator-exporter 8080:8080 &
kubectl port-forward -n cpu-cluster svc/my-cpu-cluster-soperator-exporter 8081:8080 &
```

#### æŸ¥çœ‹PrometheusæŒ‡æ ‡
```bash
# æŸ¥çœ‹GPUé›†ç¾¤æŒ‡æ ‡
curl http://localhost:8080/metrics | grep slurm

# æŸ¥çœ‹CPUé›†ç¾¤æŒ‡æ ‡
curl http://localhost:8081/metrics | grep slurm

# æŸ¥çœ‹ç³»ç»ŸæŒ‡æ ‡
curl http://localhost:8080/metrics | grep kube
```

### 6.3 é…ç½®æ—¥å¿—æ”¶é›†

#### æŸ¥çœ‹åº”ç”¨æ—¥å¿—
```bash
# æŸ¥çœ‹Slurmæ§åˆ¶å™¨æ—¥å¿—
kubectl logs -n gpu-cluster -l app.kubernetes.io/component=controller
kubectl logs -n cpu-cluster -l app.kubernetes.io/component=controller

# æŸ¥çœ‹Slurm workeræ—¥å¿—
kubectl logs -n gpu-cluster -l app.kubernetes.io/component=worker
kubectl logs -n cpu-cluster -l app.kubernetes.io/component=worker

# æŸ¥çœ‹Soperatoræ—¥å¿—
kubectl logs -n soperator-system -l app.kubernetes.io/name=soperator
```

#### æŸ¥çœ‹ç³»ç»Ÿäº‹ä»¶
```bash
# æŸ¥çœ‹æœ€è¿‘çš„é›†ç¾¤äº‹ä»¶
kubectl get events -n gpu-cluster --sort-by='.lastTimestamp' | tail -20
kubectl get events -n cpu-cluster --sort-by='.lastTimestamp' | tail -20

# æŸ¥çœ‹è­¦å‘Šäº‹ä»¶
kubectl get events --all-namespaces --field-selector type=Warning
```

## ğŸ“Š å®Œæ•´éƒ¨ç½²æµç¨‹å›¾

```mermaid
graph TB
    subgraph "é˜¶æ®µ1: è£¸æœºç¯å¢ƒå‡†å¤‡ (1-2å°æ—¶)"
        A1[ç¡¬ä»¶æ£€æŸ¥] --> A2[Ubuntu 24.04å®‰è£…]
        A2 --> A3[ç½‘ç»œé…ç½®]
        A3 --> A4[å†…æ ¸å‚æ•°é…ç½®]
        A4 --> A5[ç¦ç”¨Swap]
    end

    subgraph "é˜¶æ®µ2: Kubernetesé›†ç¾¤æ­å»º (2-3å°æ—¶)"
        B1[å®‰è£…containerd] --> B2[å®‰è£…kubeadm/kubectl/kubelet]
        B2 --> B3[MasterèŠ‚ç‚¹åˆå§‹åŒ–]
        B3 --> B4[WorkerèŠ‚ç‚¹åŠ å…¥]
        B4 --> B5[å®‰è£…Cilium CNI]
        B5 --> B6[é…ç½®NFSå­˜å‚¨]
    end

    subgraph "é˜¶æ®µ3: GPUç¯å¢ƒé…ç½® (1-2å°æ—¶)"
        C1[å®‰è£…NVIDIAé©±åŠ¨] --> C2[å®‰è£…GPU Operator]
        C2 --> C3[éªŒè¯GPUèŠ‚ç‚¹]
    end

    subgraph "é˜¶æ®µ4: Soperatoréƒ¨ç½² (30-60åˆ†é’Ÿ)"
        D1[å®‰è£…OpenKruise] --> D2[éƒ¨ç½²Soperator Operator]
        D2 --> D3[åˆ›å»ºSlurmé›†ç¾¤CR]
        D3 --> D4[é…ç½®èŠ‚ç‚¹è¿‡æ»¤å™¨]
        D4 --> D5[åˆ›å»ºå…±äº«å­˜å‚¨PVC]
    end

    subgraph "é˜¶æ®µ5: é›†ç¾¤éªŒè¯ (30åˆ†é’Ÿ)"
        E1[æ£€æŸ¥PodçŠ¶æ€] --> E2[éªŒè¯SlurmæœåŠ¡]
        E2 --> E3[SSHç™»å½•æµ‹è¯•]
        E3 --> E4[ä½œä¸šæäº¤æµ‹è¯•]
    end

    subgraph "é˜¶æ®µ6: ç›‘æ§é…ç½® (15åˆ†é’Ÿ)"
        F1[å®‰è£…å¥åº·æ£€æŸ¥] --> F2[é…ç½®Prometheus]
        F2 --> F3[è®¾ç½®å‘Šè­¦è§„åˆ™]
    end

    A5 --> B1
    B6 --> C1
    B6 --> D1
    D5 --> E1
    E4 --> F1

    style A1 fill:#e1f5fe
    style A2 fill:#e3f2fd
    style A3 fill:#e8f5e8
    style A4 fill:#f3e5f5
    style A5 fill:#fff3e0
    style B1 fill:#fce4ec
    style B2 fill:#f1f8e9
    style B3 fill:#e0f2f1
    style B4 fill:#ede7f6
    style B5 fill:#e8eaf6
    style B6 fill:#e1f5fe
    style C1 fill:#f3e5f5
    style C2 fill:#e8f5e8
    style C3 fill:#fff3e0
    style D1 fill:#fce4ec
    style D2 fill:#f1f8e9
    style D3 fill:#e0f2f1
    style D4 fill:#ede7f6
    style D5 fill:#e8eaf6
    style E1 fill:#e1f5fe
    style E2 fill:#f3e5f5
    style E3 fill:#e8f5e8
    style E4 fill:#fff3e0
    style F1 fill:#fce4ec
    style F2 fill:#f1f8e9
    style F3 fill:#e0f2f1
```

## ğŸ¯ å…³é”®é…ç½®æ–‡ä»¶ç¤ºä¾‹

### èŠ‚ç‚¹è¿‡æ»¤å™¨é…ç½®ç¤ºä¾‹

#### GPUèŠ‚ç‚¹è¿‡æ»¤å™¨
```yaml
# gpu-node-filter.yaml
apiVersion: slurm.nebius.ai/v1
kind: SlurmCluster
metadata:
  name: gpu-cluster
  namespace: gpu-cluster
spec:
  k8sNodeFilters:
    - name: gpu-nodes
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: accelerator
                    operator: Exists
                  - key: nvidia.com/gpu.present
                    operator: In
                    values: ["true"]
                  - key: node.kubernetes.io/instance-type
                    operator: In
                    values: ["gpu-enabled"]
  slurmNodes:
    worker:
      k8sNodeFilter: gpu-nodes
      size: 4
      resources:
        requests:
          nvidia.com/gpu: "1"
        limits:
          nvidia.com/gpu: "1"
    controller:
      k8sNodeFilter: gpu-nodes
      size: 1
    login:
      k8sNodeFilter: gpu-nodes
      size: 1
```

#### CPUèŠ‚ç‚¹è¿‡æ»¤å™¨
```yaml
# cpu-node-filter.yaml
apiVersion: slurm.nebius.ai/v1
kind: SlurmCluster
metadata:
  name: cpu-cluster
  namespace: cpu-cluster
spec:
  k8sNodeFilters:
    - name: cpu-nodes
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: accelerator
                    operator: DoesNotExist
                  - key: node.kubernetes.io/instance-type
                    operator: NotIn
                    values: ["gpu-enabled"]
  slurmNodes:
    worker:
      k8sNodeFilter: cpu-nodes
      size: 2
      resources:
        requests:
          cpu: "2"
          memory: "4Gi"
        limits:
          cpu: "4"
          memory: "8Gi"
    controller:
      k8sNodeFilter: cpu-nodes
      size: 1
    login:
      k8sNodeFilter: cpu-nodes
      size: 1
```

### å­˜å‚¨é…ç½®ç¤ºä¾‹

#### NFS PersistentVolume
```yaml
# nfs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-slurm
  namespace: gpu-cluster
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-csi
  nfs:
    server: <nfs-server-ip>
    path: /export/slurm
```

#### PersistentVolumeClaim
```yaml
# jail-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jail-pvc
  namespace: gpu-cluster
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: nfs-csi
  resources:
    requests:
      storage: 100Gi
```

## âš ï¸ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### ç½‘ç»œé—®é¢˜æ’æŸ¥

#### Ciliumç½‘ç»œé—®é¢˜
```bash
# æ£€æŸ¥CiliumçŠ¶æ€
cilium status

# æ£€æŸ¥Cilium PodçŠ¶æ€
kubectl get pods -n kube-system | grep cilium

# æ£€æŸ¥Ciliumé…ç½®
cilium config view

# è¿è¡Œç½‘ç»œè¿é€šæ€§æµ‹è¯•
cilium connectivity test

# æŸ¥çœ‹Ciliumæ—¥å¿—
kubectl logs -n kube-system -l k8s-app=cilium
```

#### Podé—´é€šä¿¡é—®é¢˜
```bash
# æµ‹è¯•Podé—´ç½‘ç»œè¿é€šæ€§
kubectl run test-pod-1 --image=busybox --rm -it -- /bin/sh
# åœ¨Podå†…æ‰§è¡Œ:
# nslookup kubernetes.default.svc.cluster.local
# ping 10.244.1.1

# æ£€æŸ¥NetworkPolicy
kubectl get networkpolicy -A
```

### å­˜å‚¨é—®é¢˜æ’æŸ¥

#### NFSè¿æ¥é—®é¢˜
```bash
# æ£€æŸ¥NFSæœåŠ¡å™¨çŠ¶æ€
sudo systemctl status nfs-kernel-server

# æ£€æŸ¥NFSå¯¼å‡ºé…ç½®
sudo exportfs -v

# æµ‹è¯•NFSæŒ‚è½½
sudo mkdir -p /tmp/nfs-test
sudo mount -t nfs <nfs-server-ip>:/export/slurm /tmp/nfs-test
ls -la /tmp/nfs-test
sudo umount /tmp/nfs-test
```

#### PVCé—®é¢˜æ’æŸ¥
```bash
# æ£€æŸ¥PVCçŠ¶æ€
kubectl get pvc -A

# æ£€æŸ¥PVCè¯¦æƒ…
kubectl describe pvc jail-pvc -n gpu-cluster

# æ£€æŸ¥StorageClassçŠ¶æ€
kubectl get storageclass
kubectl describe storageclass nfs-csi
```

### GPUé—®é¢˜æ’æŸ¥

#### GPUè®¾å¤‡æ£€æµ‹é—®é¢˜
```bash
# æ£€æŸ¥èŠ‚ç‚¹GPUæ ‡ç­¾
kubectl describe nodes | grep -i gpu

# æ£€æŸ¥NVIDIAé©±åŠ¨çŠ¶æ€
kubectl logs -n gpu-operator -l app.kubernetes.io/component=nvidia-device-plugin-daemonset

# åœ¨GPUèŠ‚ç‚¹æ£€æŸ¥é©±åŠ¨
ssh <gpu-node>
nvidia-smi
lsmod | grep nvidia
```

#### GPU Podè°ƒåº¦é—®é¢˜
```bash
# æ£€æŸ¥GPU Podè°ƒåº¦å¤±è´¥åŸå› 
kubectl describe pod <gpu-pod-name> -n gpu-cluster

# æ£€æŸ¥GPUèµ„æºé…é¢
kubectl describe nodes | grep -A 10 "Allocated resources"
```

### é›†ç¾¤é—®é¢˜æ’æŸ¥

#### Podå¯åŠ¨å¤±è´¥
```bash
# æŸ¥çœ‹Podè¯¦ç»†çŠ¶æ€
kubectl describe pod <pod-name> -n <namespace>

# æŸ¥çœ‹Podæ—¥å¿—
kubectl logs <pod-name> -n <namespace>
kubectl logs <pod-name> -n <namespace> --previous

# è¿›å…¥Podè°ƒè¯•
kubectl exec -it <pod-name> -n <namespace> -- bash
```

#### é›†ç¾¤çŠ¶æ€å¼‚å¸¸
```bash
# æ£€æŸ¥Slurmé›†ç¾¤çŠ¶æ€
kubectl get slurmclusters -A
kubectl describe slurmcluster <cluster-name> -n <namespace>

# æ£€æŸ¥Soperatoræ—¥å¿—
kubectl logs -n soperator-system -l app.kubernetes.io/name=soperator

# é‡å¯Soperatorç»„ä»¶
kubectl delete pod -n soperator-system -l app.kubernetes.io/name=soperator
```

## ğŸ“ˆ éƒ¨ç½²æ—¶é—´ä¼°ç®—

### å„é˜¶æ®µæ—¶é—´é¢„ä¼°

| é˜¶æ®µ | æè¿° | é¢„ä¼°æ—¶é—´ | å¤‡æ³¨ |
|------|------|----------|------|
| 1 | è£¸æœºç¯å¢ƒå‡†å¤‡ | 1-2å°æ—¶ | å–å†³äºç¡¬ä»¶å’Œç½‘ç»œé…ç½® |
| 2 | Kubernetesé›†ç¾¤æ­å»º | 2-3å°æ—¶ | åŒ…æ‹¬ç½‘ç»œå’Œå­˜å‚¨é…ç½® |
| 3 | GPUç¯å¢ƒé…ç½® | 1-2å°æ—¶ | ä»…GPUé›†ç¾¤éœ€è¦ |
| 4 | Soperatoréƒ¨ç½² | 30-60åˆ†é’Ÿ | åŒ…æ‹¬å‰ç½®ä¾èµ– |
| 5 | é›†ç¾¤éªŒè¯å’Œæµ‹è¯• | 30åˆ†é’Ÿ | åŠŸèƒ½éªŒè¯ |
| 6 | ç›‘æ§é…ç½® | 15åˆ†é’Ÿ | å¥åº·æ£€æŸ¥å’ŒæŒ‡æ ‡ |

### æ€»æ—¶é—´é¢„ä¼°

- **CPUé›†ç¾¤**: 5-8å°æ—¶
- **GPUé›†ç¾¤**: 6-10å°æ—¶

### å½±å“éƒ¨ç½²æ—¶é—´çš„å› ç´ 

- **ç½‘ç»œå¸¦å®½**: é•œåƒä¸‹è½½é€Ÿåº¦
- **ç¡¬ä»¶æ€§èƒ½**: ç¼–è¯‘å’Œå®‰è£…é€Ÿåº¦
- **ç½‘ç»œå»¶è¿Ÿ**: é›†ç¾¤èŠ‚ç‚¹é—´é€šä¿¡
- **é…ç½®å¤æ‚åº¦**: è‡ªå®šä¹‰é…ç½®æ•°é‡
- **æ•…éšœæ’æŸ¥**: é—®é¢˜å®šä½å’Œè§£å†³æ—¶é—´

## ğŸ”§ è¿ç»´å¸¸ç”¨å‘½ä»¤

### é›†ç¾¤ç®¡ç†å‘½ä»¤

#### KubernetesåŸºç¡€æ“ä½œ
```bash
# æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes -o wide

# æŸ¥çœ‹é›†ç¾¤èµ„æºä½¿ç”¨
kubectl top nodes
kubectl top pods -n <namespace>

# æŸ¥çœ‹é›†ç¾¤äº‹ä»¶
kubectl get events -A --sort-by='.lastTimestamp'

# æŸ¥çœ‹ç³»ç»Ÿèµ„æº
kubectl describe nodes
```

#### Slurmé›†ç¾¤æ“ä½œ
```bash
# æŸ¥çœ‹Slurmé›†ç¾¤çŠ¶æ€
kubectl get slurmclusters -A

# ç¼–è¾‘é›†ç¾¤é…ç½®
kubectl edit slurmcluster <cluster-name> -n <namespace>

# æ‰©å®¹é›†ç¾¤
kubectl patch slurmcluster <cluster-name> -n <namespace> \
  --type='merge' \
  -p='{"spec":{"slurmNodes":{"worker":{"size":<new-size>}}}}'

# æŸ¥çœ‹é›†ç¾¤çŠ¶æ€è¯¦æƒ…
kubectl describe slurmcluster <cluster-name> -n <namespace>
```

#### Podç®¡ç†
```bash
# æŸ¥çœ‹PodçŠ¶æ€
kubectl get pods -n <namespace> -o wide

# é‡å¯Pod
kubectl delete pod <pod-name> -n <namespace>

# æŸ¥çœ‹Podæ—¥å¿—
kubectl logs <pod-name> -n <namespace> -f

# è¿›å…¥Pod
kubectl exec -it <pod-name> -n <namespace> -- bash
```

### ç›‘æ§å’Œè°ƒè¯•

#### ç³»ç»Ÿç›‘æ§
```bash
# æŸ¥çœ‹èµ„æºä½¿ç”¨
kubectl top nodes
kubectl top pods -n <namespace>

# æŸ¥çœ‹é›†ç¾¤å¥åº·çŠ¶æ€
kubectl get componentstatuses
kubectl get cs  # ç®€åŒ–ç‰ˆæœ¬

# æŸ¥çœ‹é›†ç¾¤ä¿¡æ¯
kubectl cluster-info
kubectl version
```

#### ç½‘ç»œè°ƒè¯•
```bash
# æ£€æŸ¥CiliumçŠ¶æ€
cilium status

# æµ‹è¯•ç½‘ç»œè¿é€šæ€§
cilium connectivity test

# æŸ¥çœ‹ç½‘ç»œç­–ç•¥
kubectl get networkpolicy -A
```

#### å­˜å‚¨æ“ä½œ
```bash
# æŸ¥çœ‹å­˜å‚¨ä½¿ç”¨
kubectl get pv,pvc -A

# æŸ¥çœ‹StorageClass
kubectl get storageclass

# æŸ¥çœ‹å­˜å‚¨è¯¦æƒ…
kubectl describe pvc <pvc-name> -n <namespace>
```

## ğŸ“š ç›¸å…³æ–‡æ¡£å’Œèµ„æº

### å®˜æ–¹æ–‡æ¡£

- [Soperator GitHub Repository](https://github.com/nebius/soperator)
- [Kuberneteså®˜æ–¹æ–‡æ¡£](https://kubernetes.io/docs/)
- [Ciliumå®˜æ–¹æ–‡æ¡£](https://docs.cilium.io/)
- [NVIDIA GPU Operatoræ–‡æ¡£](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/)

### é…ç½®æ–‡ä»¶ä½ç½®

- Helm Charts: `/helm/` ç›®å½•
- APIå®šä¹‰: `/api/v1/` å’Œ `/api/v1alpha1/`
- æ§åˆ¶å™¨ä»£ç : `/internal/controller/`
- é…ç½®æ¨¡æ¿: `/internal/render/`

### æ”¯æŒå’Œç¤¾åŒº

- GitHub Issues: [Soperator Issues](https://github.com/nebius/soperator/issues)
- Slacké¢‘é“: ï¼ˆå¦‚æœ‰ï¼‰
- é‚®ä»¶åˆ—è¡¨: ï¼ˆå¦‚æœ‰ï¼‰

---

*æœ¬æ–‡æ¡£è¯¦ç»†è®°å½•äº†ä»è£¸æœºç¯å¢ƒå¼€å§‹å®Œæ•´éƒ¨ç½²Soperatorçš„å…¨è¿‡ç¨‹ï¼Œæ¶µç›–äº†ä»åŸºç¡€ç¯å¢ƒå‡†å¤‡åˆ°ç”Ÿäº§ç¯å¢ƒè¿è¡Œçš„å„ä¸ªç¯èŠ‚ã€‚å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·å‚è€ƒç›¸å…³å®˜æ–¹æ–‡æ¡£æˆ–æäº¤Issueã€‚*