# Soperatorç”¨æˆ·é€æ˜æœºåˆ¶è¯¦è§£

## æ¦‚è¿°

Soperatoré€šè¿‡åˆ›æ–°çš„æ¶æ„è®¾è®¡ï¼Œå®ç°äº†ä¼ ç»ŸSlurm HPCç³»ç»Ÿåœ¨Kubernetesä¸Šçš„å®Œå…¨å®¹å™¨åŒ–ï¼ŒåŒæ—¶ä¿æŒäº†ç”¨æˆ·ä½¿ç”¨ä¹ æƒ¯çš„å®Œå…¨é€æ˜æ€§ã€‚ç”¨æˆ·åœ¨ä½¿ç”¨Soperatoræ—¶ï¼Œæ„Ÿè§‰ä¸åˆ°åº•å±‚Kubernetesçš„å­˜åœ¨ï¼Œå°±åƒåœ¨ä½¿ç”¨ä¼ ç»Ÿçš„ç‰©ç†Slurmé›†ç¾¤ä¸€æ ·ã€‚

## ğŸ¯ æ ¸å¿ƒè®¾è®¡ç†å¿µ

### ä¸‰å±‚æŠ½è±¡æ¶æ„

```mermaid
graph TB
    subgraph "ç”¨æˆ·è§†å›¾å±‚"
        User[ç”¨æˆ·]
        TraditionalSlurm[ä¼ ç»ŸSlurmä½“éªŒ]
        SSHLogin[SSHç™»å½•]
        JobSubmit[ä½œä¸šæäº¤]
        JobMonitor[ä½œä¸šç›‘æ§]
    end

    subgraph "é€æ˜æŠ½è±¡å±‚"
        SharedFS[å…±äº«æ–‡ä»¶ç³»ç»Ÿ]
        SlurmAPI[Slurm APIæ¥å£]
        NetworkMapping[ç½‘ç»œæ˜ å°„]
        CommandTranslation[å‘½ä»¤ç¿»è¯‘]
    end

    subgraph "Kuberneteså®ç°å±‚"
        Pods[Kubernetes Pods]
        Services[K8s Services]
        PVCs[æŒä¹…å·]
        ConfigMaps[é…ç½®æ˜ å°„]
    end

    User --> TraditionalSlurm
    TraditionalSlurm --> SSHLogin
    TraditionalSlurm --> JobSubmit
    TraditionalSlurm --> JobMonitor

    SSHLogin --> SharedFS
    JobSubmit --> SlurmAPI
    JobMonitor --> NetworkMapping
    CommandTranslation --> CommandTranslation

    SharedFS --> PVCs
    SlurmAPI --> Services
    NetworkMapping --> Services
    CommandTranslation --> Pods

    style User fill:#e1f5fe
    style SharedFS fill:#e8f5e8
    style Pods fill:#fff3e0
```

## ğŸ”„ ç”¨æˆ·è§†è§’ vs åº•å±‚å®ç°å¯¹æ¯”

### ç”¨æˆ·æ“ä½œæµç¨‹ï¼ˆå®Œå…¨é€æ˜ï¼‰
```bash
# 1. ç”¨æˆ·SSHç™»å½•ï¼ˆä¸ä¼ ç»ŸSlurmç›¸åŒï¼‰
ssh soperator@login-node-ip

# 2. å‡†å¤‡ä½œä¸šè„šæœ¬ï¼ˆä¸ä¼ ç»ŸSlurmç›¸åŒï¼‰
cat > my_job.sh << EOF
#!/bin/bash
#SBATCH --ntasks=4
#SBATCH --time=01:00:00
#SBATCH --output=job_%j.out
./my_hpc_application
EOF

# 3. æäº¤ä½œä¸šï¼ˆä¸ä¼ ç»ŸSlurmç›¸åŒï¼‰
sbatch my_job.sh

# 4. æŸ¥çœ‹ä½œä¸šçŠ¶æ€ï¼ˆä¸ä¼ ç»ŸSlurmç›¸åŒï¼‰
squeue
scontrol show job <job_id>

# 5. æŸ¥çœ‹ç»“æœï¼ˆä¸ä¼ ç»ŸSlurmç›¸åŒï¼‰
cat job_12345.out
```

### åº•å±‚Kuberneteså®ç°ï¼ˆå®Œå…¨éšè—ï¼‰
```bash
# ç”¨æˆ·çœ‹ä¸åˆ°çš„åº•å±‚æ“ä½œï¼š
# 1. SSHè¿æ¥å®é™…è¿æ¥åˆ°Login Pod
# 2. ç”¨æˆ·ä¼šè¯è¢«chrootåˆ°å…±äº«æ–‡ä»¶ç³»ç»Ÿ
# 3. sbatchå‘½ä»¤é€šè¿‡slurmctldå®¹å™¨å¤„ç†
# 4. ä½œä¸šè¢«è°ƒåº¦åˆ°Worker Podæ‰§è¡Œ
# 5. æ‰€æœ‰æ“ä½œéƒ½é€šè¿‡Kubernetes APIç®¡ç†
```

## ğŸ“Š å®Œæ•´çš„æ•°æ®æµåˆ†æ

### 1. ç”¨æˆ·SSHç™»å½•æ•°æ®æµ

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant LB as LoadBalancer
    participant LoginPod as Login Pod
    participant SSHD as SSHDæœåŠ¡
    participant Jail as å…±äº«æ–‡ä»¶ç³»ç»Ÿ
    participant K8sAPI as K8s API

    User->>LB: SSHè¿æ¥ (ç«¯å£22)
    LB->>LoginPod: è½¬å‘SSHè¯·æ±‚
    SSHD->>User: SSHè®¤è¯
    SSHD->>SSHD: Chrootåˆ°/mnt/jail
    User->>Jail: æ–‡ä»¶ç³»ç»Ÿè®¿é—®
    Note over User,Jail: ç”¨æˆ·çœ‹åˆ°çš„æ˜¯ä¼ ç»ŸSlurmç¯å¢ƒï¼Œ<br/>å®Œå…¨æ„ŸçŸ¥ä¸åˆ°Kubernetesçš„å­˜åœ¨
```

**å…³é”®å®ç°æœºåˆ¶**ï¼š
- **Login Pod**: è¿è¡ŒSSHDæœåŠ¡ï¼Œé…ç½®`ChrootDirectory /mnt/jail`
- **å…±äº«æ–‡ä»¶ç³»ç»Ÿ**: æ‰€æœ‰PodæŒ‚è½½åŒä¸€ä¸ªPVCåˆ°`/mnt/jail`
- **é€æ˜è®¿é—®**: ç”¨æˆ·SSHä¼šè¯è‡ªåŠ¨è¿›å…¥å…±äº«ç¯å¢ƒï¼Œæ„ŸçŸ¥ä¸åˆ°å®¹å™¨è¾¹ç•Œ

### 2. ä½œä¸šæäº¤æµç¨‹æ•°æ®æµ

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Sbatch as sbatchå‘½ä»¤
    participant Slurmctld as slurmctldå®¹å™¨
    participant Scheduler as Slurmè°ƒåº¦å™¨
    participant WorkerPods as Worker Pods
    participant Slurmd as slurmdå®ˆæŠ¤è¿›ç¨‹
    participant K8sScheduler as K8sè°ƒåº¦å™¨

    User->>Sbatch: æ‰§è¡Œsbatch my_job.sh
    Sbatch->>Slurmctld: ä½œä¸šæäº¤è¯·æ±‚ (æ ‡å‡†Slurmåè®®)
    Slurmctld->>Scheduler: ä½œä¸šå…¥é˜Ÿ
    Scheduler->>Slurmctld: é€‰æ‹©è®¡ç®—èŠ‚ç‚¹ (Worker Pod)
    Slurmctld->>WorkerPods: åˆ†é…ä½œä¸šåˆ°ç›®æ ‡Pod
    WorkerPods->>Slurmd: ä½œä¸šæ‰§è¡Œ
    Slurmd->>User: ä½œä¸šç»“æœå†™å…¥å…±äº«æ–‡ä»¶ç³»ç»Ÿ
    Note over User,K8sScheduler: æ•´ä¸ªè¿‡ç¨‹ç”¨æˆ·åªçœ‹åˆ°Slurmäº¤äº’ï¼Œ<br/>K8så®Œå…¨é€æ˜
```

**é€æ˜æ€§å®ç°**ï¼š
- **åè®®å…¼å®¹**: ä½¿ç”¨æ ‡å‡†Slurmåè®®ï¼Œç”¨æˆ·æ— éœ€æ„ŸçŸ¥APIè½¬æ¢
- **èŠ‚ç‚¹æŠ½è±¡**: ç”¨æˆ·çœ‹åˆ°çš„æ˜¯ä¼ ç»ŸèŠ‚ç‚¹åï¼Œè€ŒéPodå
- **çŠ¶æ€åŒæ­¥**: SlurmçŠ¶æ€å®æ—¶åŒæ­¥ï¼Œç”¨æˆ·æŸ¥è¯¢è·å¾—ä¼ ç»Ÿæ ¼å¼ç»“æœ

### 3. å…±äº«æ–‡ä»¶ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    subgraph "Kuberneteså­˜å‚¨å±‚"
        PVC[PersistentVolumeClaim]
        NFS[NFS Server]
        Storage[å…±äº«å­˜å‚¨è®¾å¤‡]
    end

    subgraph "å®¹å™¨å±‚"
        LoginPod[Login Pod]
        WorkerPod1[Worker Pod 1]
        WorkerPod2[Worker Pod 2]
        WorkerPod3[Worker Pod N]
    end

    subgraph "ç”¨æˆ·è§†å›¾å±‚"
        JailFS[å…±äº«æ ¹æ–‡ä»¶ç³»ç»Ÿ /mnt/jail]
        UserHome[ç”¨æˆ·ä¸»ç›®å½• /home/user]
        JobFiles[ä½œä¸šæ–‡ä»¶ /home/user/my_job.sh]
        JobOutput[ä½œä¸šè¾“å‡º /home/user/job_12345.out]
    end

    Storage --> NFS
    NFS --> PVC
    PVC --> LoginPod
    PVC --> WorkerPod1
    PVC --> WorkerPod2
    PVC --> WorkerPod3

    LoginPod --> JailFS
    WorkerPod1 --> JailFS
    WorkerPod2 --> JailFS
    WorkerPod3 --> JailFS

    JailFS --> UserHome
    UserHome --> JobFiles
    JobFiles --> JobOutput

    style UserHome fill:#e1f5fe
    style JobFiles fill:#e8f5e8
    style JobOutput fill:#fff3e0
```

**å…±äº«æœºåˆ¶è¯¦è§£**ï¼š
- **ç»Ÿä¸€æŒ‚è½½ç‚¹**: æ‰€æœ‰Podå°†PVCæŒ‚è½½åˆ°`/mnt/jail`
- **æ–‡ä»¶ç³»ç»Ÿä¸€è‡´æ€§**: ç”¨æˆ·åœ¨ä»»ä½•èŠ‚ç‚¹çœ‹åˆ°ç›¸åŒçš„æ–‡ä»¶ç³»ç»Ÿè§†å›¾
- **æ•°æ®æ— éœ€ä¼ è¾“**: ä½œä¸šæ–‡ä»¶å’Œè¾“å‡ºéƒ½åœ¨åŒä¸€æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œæ— éœ€é¢å¤–ä¼ è¾“æ­¥éª¤

### 4. åŒå±‚è°ƒåº¦æœºåˆ¶è¯¦è§£

```mermaid
graph LR
    subgraph Kubernetesè°ƒåº¦å±‚
        K8sNodes["K8s Nodes"]
        K8sScheduler["K8s Scheduler"]
        PodPlacement["Pod Placement"]
    end

    subgraph Slurmè°ƒåº¦å±‚
        SlurmNodes["Slurm Nodes (Pods)"]
        SlurmScheduler["Slurm Scheduler"]
        JobPlacement["Job Placement"]
    end

    subgraph èµ„æºæŠ½è±¡
        K8sPods["Pod 1, Pod 2, Pod N"]
        SlurmWorkers["worker-0, worker-1, worker-N"]
    end

    K8sNodes --> K8sScheduler
    K8sScheduler --> PodPlacement
    PodPlacement --> K8sPods

    K8sPods --> SlurmNodes
    SlurmNodes --> SlurmScheduler
    SlurmScheduler --> JobPlacement
    JobPlacement --> SlurmWorkers

    style K8sScheduler fill:#e1f5fe
    style SlurmScheduler fill:#e8f5e8
```

**è°ƒåº¦åˆ†å·¥ç­–ç•¥**ï¼š
- **K8sè°ƒåº¦å™¨**: è´Ÿè´£Podåˆ°ç‰©ç†èŠ‚ç‚¹çš„è°ƒåº¦ï¼ˆåŸºç¡€è®¾æ–½å±‚ï¼‰
- **Slurmè°ƒåº¦å™¨**: è´Ÿè´£ä½œä¸šåˆ°SlurmèŠ‚ç‚¹çš„è°ƒåº¦ï¼ˆåº”ç”¨å±‚ï¼‰
- **ä¸¤å±‚è§£è€¦**: å„å¸å…¶èŒï¼Œäº’ä¸å¹²æ‰°ï¼Œç”¨æˆ·åªæ„ŸçŸ¥Slurmè°ƒåº¦

### 5. æ ¸å¿ƒæœºåˆ¶ï¼šslurmctldå¦‚ä½•"è¯¯ä»¥ä¸º"åœ¨ç®¡ç†ç‰©ç†èŠ‚ç‚¹

**å…³é”®é—®é¢˜**ï¼šSoperatoræ²¡æœ‰ä¿®æ”¹åŸç”Ÿslurmctldç¨‹åºï¼Œå¦‚ä½•è®©å®ƒæŒ‰ç…§Podè°ƒåº¦ï¼Ÿ

**ç­”æ¡ˆ**ï¼šé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„**é€‚é…å±‚**ï¼Œè®©slurmctld"ä»¥ä¸º"è‡ªå·±åœ¨ç®¡ç†ä¼ ç»Ÿçš„ç‰©ç†èŠ‚ç‚¹é›†ç¾¤ã€‚

#### 5.1 ç½‘ç»œå±‚ä¼ªè£…æœºåˆ¶ ğŸŒ

**Headless Service + DNSè§£æ**ï¼š
```yaml
# ç”Ÿæˆçš„slurm.confä¸­çš„èŠ‚ç‚¹å®šä¹‰
NodeName=gb200-0-0 NodeAddr=gb200-0-0.gb200-0.soperator.svc RealMemory=1612639
```

**è§£æè¿‡ç¨‹**ï¼š
1. slurmctldéœ€è¦è¿æ¥èŠ‚ç‚¹ `gb200-0-0.gb200-0.soperator.svc`
2. Kubernetes DNSå°†æ­¤åŸŸåè§£æä¸ºå¯¹åº”Podçš„IPåœ°å€
3. slurmctldé€šè¿‡æ ‡å‡†ç½‘ç»œåè®®è¿æ¥Podä¸­çš„slurmdè¿›ç¨‹
4. ä»slurmctldè§†è§’çœ‹ï¼Œè¿™å°±æ˜¯æ™®é€šçš„ç‰©ç†èŠ‚ç‚¹è¿æ¥

**å…³é”®å®ç°**ï¼š[`internal/render/worker/service.go:24`](internal/render/worker/service.go#L24)
```go
func RenderService(namespace, clusterName string, worker *values.SlurmWorker) corev1.Service {
    return corev1.Service{
        Spec: corev1.ServiceSpec{
            ClusterIP: "None",  // å…³é”®ï¼šHeadless Service
            // ...
        },
    }
}
```

#### 5.2 é…ç½®å±‚åŠ¨æ€ç”Ÿæˆ âš™ï¸

**slurm.confè‡ªåŠ¨ç”Ÿæˆ**ï¼š[`internal/render/common/configmap.go:118-140`](internal/render/common/configmap.go#L118-L140)

```go
func AddNodesToSlurmConfig(res *renderutils.PropertiesConfig, cluster *values.SlurmCluster) {
    for _, nodeSet := range cluster.NodeSetList.Items {
        for i := int32(0); i < nodeSet.Spec.Replicas; i++ {
            nodeName := fmt.Sprintf("%s-%d", nodeSet.Name, i)
            // å…³é”®ï¼šä½¿ç”¨KubernetesæœåŠ¡å‘ç°æœºåˆ¶
            nodeAddr := fmt.Sprintf("%s.%s.%s.svc", nodeName, nodeSet.Name, nodeSet.Namespace)
            realMemory := strconv.FormatInt(RenderRealMemorySlurmd(corev1.ResourceRequirements{Requests: nodeSet.Spec.Slurmd.Resources}), 10)
            res.AddProperty("NodeName", fmt.Sprintf(
                "%s NodeHostname=%s NodeAddr=%s RealMemory=%s %s",
                nodeName, nodeName, nodeAddr, realMemory, nodeSet.Spec.NodeConfig.Static,
            ))
        }
    }
}
```

**æ•ˆæœ**ï¼šslurmctldè¯»å–åˆ°å®Œå…¨æ ‡å‡†çš„slurm.confï¼ŒåŒ…å«æ‰€æœ‰èŠ‚ç‚¹çš„è¯¦ç»†é…ç½®

#### 5.3 Podä¸­è¿è¡ŒåŸç”Ÿslurmd ğŸ“¦

**çœŸæ­£çš„slurmdè¿›ç¨‹**ï¼š
```dockerfile
# images/worker/Dockerfile
ENTRYPOINT ["/slurmd_entrypoint.sh"]
CMD ["slurmd"]
```

**ç¯å¢ƒå˜é‡æ³¨å…¥**ï¼š[`internal/render/worker/container.go:142-214`](internal/render/worker/container.go#L142-L214)
```go
func renderSlurmdEnv(...) []corev1.EnvVar {
    envVar := []corev1.EnvVar{
        {
            Name: "K8S_POD_NAME",
            ValueFrom: &corev1.EnvVarSource{
                FieldRef: &corev1.ObjectFieldSelector{
                    FieldPath: "metadata.name",
                },
            },
        },
        {
            Name: "K8S_SERVICE_NAME",
            Value: naming.BuildServiceName(consts.ComponentTypeWorker, clusterName),
        },
        // æ›´å¤šPodä¿¡æ¯æ³¨å…¥...
    }
}
```

**åŠ¨æ€èŠ‚ç‚¹æ³¨å†Œ**ï¼š[`images/worker/slurmd_entrypoint.sh:45-54`](images/worker/slurmd_entrypoint.sh#L45-L54)
```bash
if [ "${SOPERATOR_NODE_SETS_ON}" = "true" ]; then
  echo "Running slurmd with NodeSets configuration from slurm.conf"
else
  echo "Running slurmd with dynamic node configuration"
  slurmd_args+=(
    -Z
    --conf
    "NodeHostname=${K8S_POD_NAME} NodeAddr=${K8S_POD_NAME}.${K8S_SERVICE_NAME}.${K8S_POD_NAMESPACE}.svc RealMemory=${SLURM_REAL_MEMORY} Gres=${GRES} $(feature_conf)"
  )
fi
```

#### 5.4 å¯åŠ¨é¡ºåºåè°ƒ ğŸ”„

**å°±ç»ªç­‰å¾…æœºåˆ¶**ï¼š[`images/worker/wait-for-controller.sh:18-25`](images/worker/wait-for-controller.sh#L18-L25)
```bash
if scontrol_output=$(scontrol ping 2>&1); then
    echo "Controller is ready!"
    exit 0
fi
```

**åˆå§‹åŒ–å®¹å™¨**ï¼š[`internal/render/worker/statefulset.go:52-56`](internal/render/worker/statefulset.go#L52-L56)
```go
initContainers := []corev1.Container{
    common.RenderContainerMunge(&worker.ContainerMunge),
    RenderContainerWaitForController(&worker.ContainerSlurmd), // ç­‰å¾…æ§åˆ¶å™¨å°±ç»ª
}
```

#### 5.5 å®Œæ•´çš„"æ¬ºéª—"é“¾æ¡ ğŸ¯

```mermaid
sequenceDiagram
    participant S as slurmctld (åŸç”Ÿ)
    participant D as Kubernetes DNS
    participant H as Headless Service
    participant P as Worker Pod
    participant SL as slurmd (åŸç”Ÿ)

    S->>D: è§£æ gb200-0-0.gb200-0.soperator.svc
    D->>H: æŸ¥è¯¢Serviceè®°å½•
    H->>P: è¿”å›Pod IPåœ°å€
    D->>S: è¿”å›Pod IP
    S->>SL: å»ºç«‹æ ‡å‡†slurmdè¿æ¥
    SL->>S: å“åº”èŠ‚ç‚¹çŠ¶æ€
    Note over S,SL: slurmctldä»¥ä¸ºåœ¨è¿æ¥ç‰©ç†èŠ‚ç‚¹
```

#### 5.6 å…³é”®æŠ€æœ¯æ€»ç»“

**é€‚é…å±‚ç»„ä»¶**ï¼š
- **ç½‘ç»œé€‚é…**ï¼šHeadless Service + Kubernetes DNS
- **é…ç½®é€‚é…**ï¼šåŠ¨æ€ç”Ÿæˆæ ‡å‡†slurm.conf
- **è¿›ç¨‹é€‚é…**ï¼šPodä¸­è¿è¡ŒåŸç”Ÿslurmd
- **å­˜å‚¨é€‚é…**ï¼šå…±äº«PVC + chrootæœºåˆ¶
- **æ—¶åºé€‚é…**ï¼šinitå®¹å™¨ç¡®ä¿å¯åŠ¨é¡ºåº

**"é”™è§‰"çš„å®ç°**ï¼š
1. slurmctldçœ‹åˆ°æ ‡å‡†çš„slurm.confé…ç½®
2. é€šè¿‡DNSå¯ä»¥è§£æåˆ°"èŠ‚ç‚¹"IPåœ°å€
3. å¯ä»¥ä¸"èŠ‚ç‚¹"ä¸Šçš„slurmdæ­£å¸¸é€šä¿¡
4. å…±äº«æ–‡ä»¶ç³»ç»Ÿè®©ä½œä¸šæ­£å¸¸æ‰§è¡Œ

**æ ¸å¿ƒæ´å¯Ÿ**ï¼šSoperatoræ²¡æœ‰ä¿®æ”¹ä»»ä½•Slurmæ ¸å¿ƒç»„ä»¶ï¼Œè€Œæ˜¯é€šè¿‡KubernetesåŸç”Ÿæœºåˆ¶æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„**èŠ‚ç‚¹æ¨¡æ‹Ÿç¯å¢ƒ**ï¼Œè®©åŸç”Ÿslurmctld"æ— ç¼"åœ°åœ¨Podç¯å¢ƒä¸­è¿è¡Œã€‚

## ğŸ”§ å…³é”®æŠ€æœ¯å®ç°è¯¦è§£

### 1. SPANKæ’ä»¶å®ç°é€æ˜éš”ç¦»

**æ–‡ä»¶ä½ç½®**: `images/common/chroot-plugin/chroot.c`

```c
// å…³é”®ä»£ç ç‰‡æ®µï¼šSPANKæ’ä»¶åˆå§‹åŒ–
int slurm_spank_init(spank_t sp, int ac, char **av) {
    // æ³¨å†Œä½œä¸šåˆå§‹åŒ–å›è°ƒ
    spank_register_callback(sp, S_JOB_INIT, job_init_callback);
    return 0;
}

// ä½œä¸šåˆå§‹åŒ–æ—¶çš„chrootæ“ä½œ
int job_init_callback(spank_t sp, int ac, char **av) {
    // 1. åˆ›å»ºæ–°çš„mount namespace
    unshare(CLONE_NEWNS);

    // 2. ç§æœ‰åŒ–å½“å‰mount namespace
    mount("none", "/", NULL, MS_REC|MS_PRIVATE, NULL);

    // 3. ç»‘å®šæŒ‚è½½å…³é”®ç›®å½•
    mount("proc", "/mnt/jail/proc", "proc", MS_NOSUID|MS_NOEXEC|MS_NODEV, NULL);
    mount("sysfs", "/mnt/jail/sys", "sysfs", MS_NOSUID|MS_NOEXEC|MS_NODEV, NULL);

    // 4. åˆ‡æ¢æ ¹ç›®å½•
    pivot_root("/mnt/jail", "/mnt/jail/.pivot_root");

    // 5. è¿›å…¥æ–°çš„æ ¹ç›®å½•
    chroot(".");
    chdir("/");

    return 0;
}
```

**é€æ˜æ•ˆæœ**ï¼š
- **è‡ªåŠ¨éš”ç¦»**: ä½œä¸šè¿è¡Œæ—¶è‡ªåŠ¨è¿›å…¥å…±äº«æ–‡ä»¶ç³»ç»Ÿç¯å¢ƒ
- **ç»Ÿä¸€è§†å›¾**: ç”¨æˆ·å’Œä½œä¸šçœ‹åˆ°çš„éƒ½æ˜¯ç»Ÿä¸€çš„æ–‡ä»¶ç³»ç»Ÿè§†å›¾
- **ç³»ç»Ÿéš”ç¦»**: ç³»ç»Ÿç›®å½•ï¼ˆ/proc, /sysç­‰ï¼‰ä»ä¿æŒéš”ç¦»ï¼Œç¡®ä¿å®‰å…¨

### 2. å®¹å™¨ç½‘ç»œé€æ˜åŒ–

**DNSé…ç½®ç­–ç•¥**ï¼š
```yaml
# Worker Podçš„DNSé…ç½®
dnsConfig:
  searches:
    - worker-0.slurm-cluster.gpu-cluster.svc.cluster.local
    - login.slurm-cluster.gpu-cluster.svc.cluster.local
  nameservers:
    - 10.96.0.10  # K8s DNSæœåŠ¡
```

**æœåŠ¡å‘ç°æœºåˆ¶**ï¼š
```yaml
# Slurmæ§åˆ¶å™¨æœåŠ¡
apiVersion: v1
kind: Service
metadata:
  name: slurm-cluster-controller
spec:
  selector:
    app.kubernetes.io/component: controller
  ports:
    - port: 6817  # slurmctldç«¯å£
      targetPort: 6817
```

**ç½‘ç»œé€æ˜æ•ˆæœ**ï¼š
- **Podé—´é€šä¿¡**: é€šè¿‡K8sæœåŠ¡å‘ç°é€šä¿¡ï¼Œç”¨æˆ·çœ‹åˆ°ä¼ ç»ŸèŠ‚ç‚¹å
- **ç«¯å£æ˜ å°„**: æš´éœ²æ ‡å‡†Slurmç«¯å£ï¼Œä¿æŒåè®®å…¼å®¹æ€§
- **DNSè§£æ**: ä¼ ç»ŸèŠ‚ç‚¹åè‡ªåŠ¨è§£æåˆ°Pod IPï¼Œç”¨æˆ·æ— æ„ŸçŸ¥

### 3. ä½œä¸šæ‰§è¡Œç¯å¢ƒä¸€è‡´æ€§

**å®¹å™¨å¯åŠ¨è„šæœ¬**: `images/worker/slurmd_entrypoint.sh`

```bash
#!/bin/bash

# 1. ç­‰å¾…å…±äº«æ–‡ä»¶ç³»ç»Ÿå°±ç»ª
while [ ! -d /mnt/jail ]; do
    echo "Waiting for jail filesystem..."
    sleep 1
done

# 2. ç»‘å®šæŒ‚è½½SlurmäºŒè¿›åˆ¶æ–‡ä»¶åˆ°jail
mount --bind /usr/bin/slurmd /mnt/jail/usr/bin/slurmd
mount --bind /usr/lib/slurm /mnt/jail/usr/lib/slurm

# 3. å¯åŠ¨slurmdå®ˆæŠ¤è¿›ç¨‹
exec /usr/sbin/slurmd -D -S /var/run/slurmd/slurmd.sock
```

**ç”¨æˆ·ç¯å¢ƒä¸€è‡´æ€§éªŒè¯**ï¼š
```bash
# ç”¨æˆ·SSHç™»å½•åçœ‹åˆ°çš„ç¯å¢ƒ
$ which sbatch
/usr/bin/sbatch  # å®é™…ä½äº/mnt/jail/usr/bin/sbatch

$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
gpu*         up   infinite      4  idle  worker-[0-3]

$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
             12345    gpu*  my_job   user    R       0:10      1 worker-2
```

## ğŸ¯ é€æ˜æ€§å®ç°çš„å…³é”®è¦ç´ 

### 1. æ–‡ä»¶ç³»ç»Ÿé€æ˜æ€§

| å®ç°æœºåˆ¶ | é€æ˜æ•ˆæœ | ç”¨æˆ·æ„ŸçŸ¥ |
|----------|----------|----------|
| **ç»Ÿä¸€è§†å›¾** | æ‰€æœ‰èŠ‚ç‚¹è®¿é—®åŒä¸€æ–‡ä»¶ç³»ç»Ÿ | âœ… çœ‹åˆ°ä¼ ç»Ÿæ–‡ä»¶ç³»ç»Ÿ |
| **è·¯å¾„ä¸€è‡´æ€§** | ä¸ä¼ ç»ŸSlurmç›¸åŒçš„è·¯å¾„ç»“æ„ | âœ… è·¯å¾„å®Œå…¨ç›¸åŒ |
| **æƒé™ä¸€è‡´æ€§** | ä¿ç•™æ ‡å‡†UNIXæƒé™æ¨¡å‹ | âœ… æƒé™ç®¡ç†æ— å˜åŒ– |

### 2. ç½‘ç»œé€æ˜æ€§

| å®ç°æœºåˆ¶ | é€æ˜æ•ˆæœ | ç”¨æˆ·æ„ŸçŸ¥ |
|----------|----------|----------|
| **æœåŠ¡å‘ç°** | ä½¿ç”¨ä¼ ç»Ÿä¸»æœºåè€ŒéK8sæœåŠ¡å | âœ… ä¸»æœºåæ— å˜åŒ– |
| **ç«¯å£æ˜ å°„** | æš´éœ²æ ‡å‡†Slurmç«¯å£ | âœ… ç«¯å£è®¿é—®æ— å˜åŒ– |
| **DNSè§£æ** | è‡ªåŠ¨è§£æèŠ‚ç‚¹ååˆ°Pod IP | âœ… ç½‘ç»œè®¿é—®é€æ˜ |

### 3. å‘½ä»¤é€æ˜æ€§

| å®ç°æœºåˆ¶ | é€æ˜æ•ˆæœ | ç”¨æˆ·æ„ŸçŸ¥ |
|----------|----------|----------|
| **äºŒè¿›åˆ¶å…¼å®¹** | æ ‡å‡†Slurmå‘½ä»¤ | âœ… å‘½ä»¤å®Œå…¨ç›¸åŒ |
| **å‚æ•°å…¼å®¹** | æ‰€æœ‰sbatch/srunå‚æ•°å®Œå…¨ç›¸åŒ | âœ… å‚æ•°æ— å˜åŒ– |
| **è¾“å‡ºæ ¼å¼** | ä¸ä¼ ç»ŸSlurmç›¸åŒçš„è¾“å‡ºæ ¼å¼ | âœ… è¾“å‡ºæ ¼å¼ä¸€è‡´ |

### 4. çŠ¶æ€é€æ˜æ€§

| å®ç°æœºåˆ¶ | é€æ˜æ•ˆæœ | ç”¨æˆ·æ„ŸçŸ¥ |
|----------|----------|----------|
| **ä½œä¸šçŠ¶æ€** | å®æ—¶åŒæ­¥Slurmä½œä¸šçŠ¶æ€ | âœ… çŠ¶æ€æŸ¥è¯¢æ— å˜åŒ– |
| **èŠ‚ç‚¹çŠ¶æ€** | æ˜¾ç¤ºä¼ ç»ŸèŠ‚ç‚¹ä¿¡æ¯è€ŒéPodä¿¡æ¯ | âœ… èŠ‚ç‚¹è§†å›¾ä¸€è‡´ |
| **èµ„æºçŠ¶æ€** | æ˜¾ç¤ºCPU/GPUä½¿ç”¨æƒ…å†µ | âœ… èµ„æºç›‘æ§æ— å˜åŒ– |

## ğŸš€ æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åˆ†æ

### 1. å…±äº«æ ¹æ–‡ä»¶ç³»ç»Ÿæ¶æ„

**åˆ›æ–°ç‚¹**: é€šè¿‡OverlayFS + ç»‘å®šæŒ‚è½½ï¼Œåœ¨å®¹å™¨ç¯å¢ƒä¸­é‡ç°ä¼ ç»ŸSlurmé›†ç¾¤çš„æ–‡ä»¶ç³»ç»Ÿä½“éªŒ

**æŠ€æœ¯å®ç°æ¶æ„**ï¼š
```mermaid
graph TB
    subgraph "ç‰©ç†å­˜å‚¨å±‚"
        NFS_Server[NFSæœåŠ¡å™¨]
        Physical_Storage[ç‰©ç†å­˜å‚¨è®¾å¤‡]
    end

    subgraph "KubernetesæŠ½è±¡å±‚"
        PVC[PersistentVolumeClaim]
        StorageClass[StorageClass]
    end

    subgraph "å®¹å™¨æŒ‚è½½å±‚"
        LowerDir[Lower Directory - åŸºç¡€é•œåƒ]
        UpperDir[Upper Directory - è¿è¡Œæ—¶ä¿®æ”¹]
        WorkDir[Work Directory - OverlayFSå·¥ä½œåŒº]
        MergedDir[Merged Directory - ç”¨æˆ·çœ‹åˆ°çš„æ–‡ä»¶ç³»ç»Ÿ]
    end

    Physical_Storage --> NFS_Server
    NFS_Server --> StorageClass
    StorageClass --> PVC

    LowerDir --> MergedDir
    UpperDir --> MergedDir
    WorkDir --> MergedDir
    PVC --> UpperDir
    PVC --> MergedDir

    style MergedDir fill:#e1f5fe
    style UpperDir fill:#e8f5e8
    style LowerDir fill:#fff3e0
```

**é€æ˜æ€§ä¼˜åŠ¿**ï¼š
- âœ… ç”¨æˆ·çœ‹åˆ°ç»Ÿä¸€çš„æ–‡ä»¶ç³»ç»Ÿè§†å›¾
- âœ… ä½œä¸šæ•°æ®æ— éœ€é¢å¤–ä¼ è¾“
- âœ… æ”¯æŒä¼ ç»ŸSlurmçš„å…±äº«æ–‡ä»¶ç³»ç»Ÿä¾èµ–
- âœ… å®Œå…¨ä¿æŒæ–‡ä»¶ç³»ç»Ÿä½¿ç”¨ä¹ æƒ¯

### 2. åŒå±‚è°ƒåº¦è§£è€¦æœºåˆ¶

**åˆ›æ–°ç‚¹**: Kubernetesè°ƒåº¦å™¨ç®¡ç†åŸºç¡€è®¾æ–½ï¼ŒSlurmè°ƒåº¦å™¨ç®¡ç†ä½œä¸šï¼Œä¸¤å±‚å®Œå…¨è§£è€¦

**è°ƒåº¦æµç¨‹å¯¹æ¯”**ï¼š

| è°ƒåº¦å±‚çº§ | ä¼ ç»ŸSlurm | Soperator |
|----------|-----------|-----------|
| **èµ„æºç®¡ç†** | ç‰©ç†èŠ‚ç‚¹ | Kubernetes Pods |
| **è°ƒåº¦å™¨** | slurmctld | K8s Scheduler + slurmctld |
| **è°ƒåº¦å•ä½** | ç‰©ç†èŠ‚ç‚¹ | Podï¼ˆåŸºç¡€è®¾æ–½ï¼‰â†’ SlurmèŠ‚ç‚¹ï¼ˆä½œä¸šï¼‰ |
| **æ‰©å±•æ€§** | æ‰‹åŠ¨æ·»åŠ èŠ‚ç‚¹ | è‡ªåŠ¨æ‰©ç¼©å®¹Pod |
| **ç”¨æˆ·æ„ŸçŸ¥** | ç›´æ¥æ„ŸçŸ¥ç‰©ç†èŠ‚ç‚¹ | æ„ŸçŸ¥è™šæ‹ŸèŠ‚ç‚¹ï¼ŒPodå®Œå…¨é€æ˜ |

### 3. SPANKæ’ä»¶éš”ç¦»æœºåˆ¶

**åˆ›æ–°ç‚¹**: é€šè¿‡SPANKæ’ä»¶åœ¨ä½œä¸šè¿è¡Œæ—¶åŠ¨æ€åˆ›å»ºéš”ç¦»ç¯å¢ƒï¼Œå®ç°"å®¹å™¨çš„å®¹å™¨åŒ–"

**SPANKæ’ä»¶å·¥ä½œæµç¨‹**ï¼š
```mermaid
sequenceDiagram
    participant Slurmd as slurmdå®ˆæŠ¤è¿›ç¨‹
    participant Spank as SPANKæ’ä»¶
    participant Kernel as Linuxå†…æ ¸
    participant Jail as Jailç¯å¢ƒ

    Slurmd->>Spank: ä½œä¸šåˆå§‹åŒ–
    Spank->>Kernel: unshare(CLONE_NEWNS)
    Spank->>Kernel: mount(MS_PRIVATE)
    Spank->>Jail: bind mountç³»ç»Ÿç›®å½•
    Spank->>Kernel: pivot_root()
    Spank->>Jail: chroot(/mnt/jail)
    Spank->>Slurmd: æ‰§è¡Œç”¨æˆ·ä½œä¸š
    Note over Slurmd,Jail: ä½œä¸šåœ¨éš”ç¦»çš„jailç¯å¢ƒä¸­è¿è¡Œï¼Œ<br/>ç”¨æˆ·æ„ŸçŸ¥ä¸åˆ°å®¹å™¨çš„å­˜åœ¨
```

## ğŸ”„ å®Œæ•´çš„ç”¨æˆ·ä½œä¸šç”Ÿå‘½å‘¨æœŸ

### ä»ç”¨æˆ·è§†è§’çš„å®Œæ•´æµç¨‹

```mermaid
journey
    title ç”¨æˆ·ä½œä¸šæäº¤æµç¨‹
    section SSHç™»å½•
      ç”¨æˆ·è¿æ¥: 5: ç”¨æˆ·
      SSHè®¤è¯: 5: ç³»ç»Ÿ
      è¿›å…¥ç¯å¢ƒ: 5: ç”¨æˆ·
    section ä½œä¸šå‡†å¤‡
      ç¼–å†™è„šæœ¬: 5: ç”¨æˆ·
      æµ‹è¯•è„šæœ¬: 4: ç”¨æˆ·
    section ä½œä¸šæäº¤
      æäº¤ä½œä¸š: 5: ç”¨æˆ·
      è·å–ä½œä¸šID: 5: ç”¨æˆ·
    section ä½œä¸šç›‘æ§
      æŸ¥çœ‹é˜Ÿåˆ—: 5: ç”¨æˆ·
      æ£€æŸ¥çŠ¶æ€: 5: ç”¨æˆ·
    section ç»“æœè·å–
      ä½œä¸šå®Œæˆ: 5: ç”¨æˆ·
      æŸ¥çœ‹è¾“å‡º: 5: ç”¨æˆ·
    section ä½“éªŒè¯„ä»·
      æ“ä½œä¹ æƒ¯: 5: ç”¨æˆ·
      å­¦ä¹ æˆæœ¬: 5: ç”¨æˆ·
      æ»¡æ„åº¦: 5: ç”¨æˆ·
```

### åº•å±‚ç³»ç»Ÿå¯¹åº”çš„æ“ä½œ

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant System as åº•å±‚ç³»ç»Ÿ
    participant K8s as Kubernetes

    Note over User,K8s: 1. SSHç™»å½•é˜¶æ®µ
    User->>System: ssh soperator@login-node
    System->>K8s: è¿æ¥åˆ°Login Pod
    K8s->>System: æ‰§è¡ŒChrootåˆ°/mnt/jail
    System->>User: è¿”å›ä¼ ç»ŸSlurmç¯å¢ƒ

    Note over User,K8s: 2. ä½œä¸šæäº¤é˜¶æ®µ
    User->>System: sbatch job.sh
    System->>K8s: é€šè¿‡slurmctldå®¹å™¨å¤„ç†
    K8s->>System: è°ƒåº¦ä½œä¸šåˆ°Worker Pod
    System->>User: è¿”å›ä½œä¸šID

    Note over User,K8s: 3. ä½œä¸šæ‰§è¡Œé˜¶æ®µ
    System->>K8s: åœ¨Worker Podä¸­æ‰§è¡Œä½œä¸š
    K8s->>System: é€šè¿‡SPANKæ’ä»¶éš”ç¦»ç¯å¢ƒ
    System->>User: ä½œä¸šåœ¨å…±äº«æ–‡ä»¶ç³»ç»Ÿæ‰§è¡Œ

    Note over User,K8s: 4. ç»“æœè®¿é—®é˜¶æ®µ
    User->>System: cat job_12345.out
    System->>K8s: è®¿é—®å…±äº«æ–‡ä»¶ç³»ç»Ÿ
    K8s->>User: è¿”å›ä½œä¸šè¾“å‡º
```

## ğŸ“ˆ ä¸ä¼ ç»ŸSlurmçš„å¯¹æ¯”ä¼˜åŠ¿

### é€æ˜æ€§ä¿æŒåº¦å¯¹æ¯”

| ä½“éªŒç»´åº¦ | ä¼ ç»ŸSlurmé›†ç¾¤ | Soperator | é€æ˜åº¦è¯„åˆ† |
|----------|---------------|-----------|------------|
| **SSHç™»å½•** | ç›´æ¥ç™»å½•ç‰©ç†èŠ‚ç‚¹ | ç™»å½•Login Pod | â­â­â­â­â­ |
| **å‘½ä»¤ä½¿ç”¨** | æ ‡å‡†Slurmå‘½ä»¤ | å®Œå…¨å…¼å®¹ | â­â­â­â­â­ |
| **æ–‡ä»¶è®¿é—®** | æœ¬åœ°/å…±äº«æ–‡ä»¶ç³»ç»Ÿ | å…±äº«æ–‡ä»¶ç³»ç»Ÿ | â­â­â­â­â­ |
| **ä½œä¸šæäº¤** | sbatch/srun | å®Œå…¨å…¼å®¹ | â­â­â­â­â­ |
| **çŠ¶æ€æŸ¥è¯¢** | squeue/sinfo | å®Œå…¨å…¼å®¹ | â­â­â­â­â­ |
| **ç¯å¢ƒå˜é‡** | æ ‡å‡†Slurmç¯å¢ƒå˜é‡ | å®Œå…¨å…¼å®¹ | â­â­â­â­â­ |
| **é…ç½®æ–‡ä»¶** | æ ‡å‡†slurm.conf | å®Œå…¨å…¼å®¹ | â­â­â­â­â­ |

### è¿ç»´ä¼˜åŠ¿å¯¹æ¯”

| ç»´åº¦ | ä¼ ç»ŸSlurmé›†ç¾¤ | Soperator |
|------|---------------|-----------|
| **éƒ¨ç½²å¤æ‚åº¦** | æ‰‹åŠ¨é…ç½®ï¼Œå¤æ‚åº¦é«˜ | è‡ªåŠ¨åŒ–éƒ¨ç½²ï¼Œç®€å•å¿«é€Ÿ |
| **æ‰©å±•æ€§** | æ‰‹åŠ¨æ·»åŠ èŠ‚ç‚¹ï¼Œè€—æ—¶ | è‡ªåŠ¨æ‰©ç¼©å®¹ï¼Œåˆ†é’Ÿçº§ |
| **æ•…éšœæ¢å¤** | æ‰‹åŠ¨æ’æŸ¥å’Œä¿®å¤ | è‡ªæ„ˆèƒ½åŠ›ï¼Œè‡ªåŠ¨æ¢å¤ |
| **èµ„æºåˆ©ç”¨ç‡** | é™æ€åˆ†é…ï¼Œåˆ©ç”¨ç‡ä½ | åŠ¨æ€è°ƒåº¦ï¼Œåˆ©ç”¨ç‡é«˜ |
| **è¿ç»´æˆæœ¬** | é«˜ï¼Œéœ€ä¸“èŒè¿ç»´ | ä½ï¼Œè‡ªåŠ¨åŒ–è¿ç»´ |
| **ç›‘æ§èƒ½åŠ›** | åŸºç¡€ç›‘æ§ | å…¨æ–¹ä½ç›‘æ§å’Œå‘Šè­¦ |
| **å¤šç§Ÿæˆ·** | å¤æ‚çš„æƒé™ç®¡ç† | åŸç”ŸK8så¤šç§Ÿæˆ·æ”¯æŒ |

## ğŸ” ç›‘æ§å’Œå¯è§‚æµ‹æ€§

### åŒå±‚ç›‘æ§ä½“ç³»

```mermaid
graph TB
    subgraph "ç”¨æˆ·å¯è§ç›‘æ§"
        SlurmQueue[Slurmé˜Ÿåˆ—çŠ¶æ€]
        JobStatus[ä½œä¸šçŠ¶æ€]
        NodeStatus[èŠ‚ç‚¹çŠ¶æ€]
        ResourceUsage[èµ„æºä½¿ç”¨æƒ…å†µ]
    end

    subgraph "åº•å±‚ç³»ç»Ÿç›‘æ§"
        PodHealth[Podå¥åº·çŠ¶æ€]
        K8sEvents[K8säº‹ä»¶]
        StorageHealth[å­˜å‚¨å¥åº·]
        NetworkHealth[ç½‘ç»œå¥åº·]
    end

    subgraph "é›†æˆç›‘æ§"
        Prometheus[PrometheusæŒ‡æ ‡]
        GrafanaDashboard[Grafanaä»ªè¡¨æ¿]
        Alerting[å‘Šè­¦ç³»ç»Ÿ]
    end

    SlurmQueue --> Prometheus
    JobStatus --> Prometheus
    NodeStatus --> Prometheus
    ResourceUsage --> Prometheus

    PodHealth --> Prometheus
    K8sEvents --> Prometheus
    StorageHealth --> Prometheus
    NetworkHealth --> Prometheus

    Prometheus --> GrafanaDashboard
    Prometheus --> Alerting

    style SlurmQueue fill:#e1f5fe
    style PodHealth fill:#e8f5e8
    style Prometheus fill:#fff3e0
```

### æŒ‡æ ‡é›†æˆå®ç°

```go
// PrometheusæŒ‡æ ‡å¯¼å‡ºå®ç°
func (e *ClusterExporter) CollectJobsMetrics() {
    jobs, err := e.client.ListJobs(ctx)
    for _, job := range jobs {
        // ä½œä¸šæ•°é‡ç»Ÿè®¡
        prometheusJobsTotal.WithLabelValues(job.Partition).Inc()

        // ä½œä¸šè¿è¡Œæ—¶é—´
        prometheusJobsRuntime.WithLabelValues(job.State).Set(job.Runtime)

        // ä½œä¸šèµ„æºä½¿ç”¨
        prometheusJobsCPU.WithLabelValues(job.JobID).Set(job.CPUUsage)
        prometheusJobsMemory.WithLabelValues(job.JobID).Set(job.MemoryUsage)
    }
}

// èŠ‚ç‚¹çŠ¶æ€æŒ‡æ ‡
func (e *ClusterExporter) CollectNodesMetrics() {
    nodes, err := e.client.ListNodes(ctx)
    for _, node := range nodes {
        // èŠ‚ç‚¹çŠ¶æ€ç»Ÿè®¡
        prometheusNodesState.WithLabelValues(node.State).Inc()

        // èŠ‚ç‚¹èµ„æºä½¿ç”¨
        prometheusNodesCPU.WithLabelValues(node.Name).Set(node.CPUAlloc)
        prometheusNodesMemory.WithLabelValues(node.Name).Set(node.MemoryAlloc)
    }
}
```

## ğŸ¯ æœ€ä½³å®è·µå»ºè®®

### 1. ä¿æŒé€æ˜æ€§çš„é…ç½®åŸåˆ™

#### æ–‡ä»¶ç³»ç»Ÿé…ç½®
```yaml
# ä¿æŒä¼ ç»Ÿè·¯å¾„ç»“æ„
volumeMounts:
  - name: jail
    mountPath: /mnt/jail
  - name: slurm-bin
    mountPath: /usr/bin/slurm-bin
```

#### ç½‘ç»œé…ç½®
```yaml
# ä¿æŒä¼ ç»Ÿä¸»æœºåè§£æ
dnsConfig:
  searches:
    - worker.slurm-cluster.local  # ä¼ ç»Ÿæ ¼å¼
    - login.slurm-cluster.local
```

#### ç¯å¢ƒå˜é‡é…ç½®
```bash
# ä¿æŒæ ‡å‡†Slurmç¯å¢ƒå˜é‡
export SLURM_CONF=/etc/slurm/slurm.conf
export SLURM_JOB_ID=$SLURM_JOB_ID
export SLURM_NTASKS=$SLURM_NTASKS
```

### 2. ç”¨æˆ·ä½“éªŒéªŒè¯æ£€æŸ¥æ¸…å•

- [ ] SSHç™»å½•åæ˜¾ç¤ºä¼ ç»ŸSlurmç¯å¢ƒæç¤º
- [ ] `which sbatch` è¿”å›æ ‡å‡†è·¯å¾„ `/usr/bin/sbatch`
- [ ] `sinfo` æ˜¾ç¤ºä¼ ç»ŸèŠ‚ç‚¹åˆ—è¡¨æ ¼å¼
- [ ] `squeue` æ˜¾ç¤ºæ ‡å‡†ä½œä¸šé˜Ÿåˆ—æ ¼å¼
- [ ] ä½œä¸šæäº¤åè·å¾—æ ‡å‡†ä½œä¸šIDæ ¼å¼
- [ ] ä½œä¸šè¾“å‡ºæ–‡ä»¶ä½ç½®ä¸ä¼ ç»Ÿä¸€è‡´
- [ ] ç¯å¢ƒå˜é‡ `$SLURM_*` æ­£å¸¸å·¥ä½œ
- [ ] æ‰€æœ‰æ ‡å‡†Slurmå‘½ä»¤æ­£å¸¸æ‰§è¡Œ

### 3. æ•…éšœæ’æŸ¥æŒ‡å—

#### é€æ˜æ€§é—®é¢˜æ’æŸ¥
```bash
# 1. æ£€æŸ¥å…±äº«æ–‡ä»¶ç³»ç»ŸæŒ‚è½½
kubectl exec -it login-0 -n cluster -- df -h | grep jail

# 2. éªŒè¯SlurmäºŒè¿›åˆ¶æ–‡ä»¶å¯è®¿é—®æ€§
kubectl exec -it login-0 -n cluster -- which sbatch

# 3. æ£€æŸ¥èŠ‚ç‚¹æ³¨å†ŒçŠ¶æ€
kubectl exec -it login-0 -n cluster -- sinfo

# 4. éªŒè¯ä½œä¸šè°ƒåº¦åŠŸèƒ½
kubectl exec -it login-0 -n cluster -- sbatch --test-only test_job.sh
```

#### ç½‘ç»œé€æ˜æ€§æ’æŸ¥
```bash
# 1. æ£€æŸ¥DNSè§£æ
kubectl exec -it login-0 -n cluster -- nslookup worker-0

# 2. éªŒè¯æœåŠ¡è¿é€šæ€§
kubectl exec -it login-0 -n cluster -- telnet slurmctld 6817

# 3. æ£€æŸ¥èŠ‚ç‚¹é—´é€šä¿¡
kubectl exec -it worker-0 -n cluster -- ping worker-1
```

## ğŸ¯ æ€»ç»“ï¼šé€æ˜æ€§å®ç°çš„æ ¸å¿ƒä»·å€¼

### ä¸‰å¤§æ”¯æŸ±å®ç°å®Œå…¨é€æ˜

1. **å…±äº«æ ¹æ–‡ä»¶ç³»ç»Ÿ**: é€šè¿‡OverlayFS + SPANKæ’ä»¶ï¼Œåœ¨å®¹å™¨ä¸­é‡ç°ä¼ ç»Ÿæ–‡ä»¶ç³»ç»Ÿä½“éªŒ
2. **åŒå±‚è°ƒåº¦è§£è€¦**: K8sè°ƒåº¦å™¨ç®¡ç†åŸºç¡€è®¾æ–½ï¼ŒSlurmè°ƒåº¦å™¨ç®¡ç†ä½œä¸šï¼Œå„å¸å…¶èŒ
3. **ç½‘ç»œæ˜ å°„é€æ˜**: ä¼ ç»Ÿä¸»æœºåè‡ªåŠ¨æ˜ å°„åˆ°K8sæœåŠ¡ï¼Œç½‘ç»œè®¿é—®å®Œå…¨é€æ˜

### ç”¨æˆ·ä½“éªŒé›¶å˜åŒ–

- âœ… **å­¦ä¹ æˆæœ¬ä¸ºé›¶**: æ— éœ€å­¦ä¹ æ–°çš„å·¥å…·å’Œå‘½ä»¤
- âœ… **è¿ç§»æˆæœ¬ä¸ºé›¶**: ç°æœ‰è„šæœ¬å’Œæµç¨‹å®Œå…¨å…¼å®¹
- âœ… **æ“ä½œä¹ æƒ¯ä¸ºé›¶å˜åŒ–**: æ‰€æœ‰æ“ä½œæ–¹å¼ä¿æŒä¸€è‡´
- âœ… **å¿ƒæ™ºæ¨¡å‹é›¶æ”¹å˜**: ç”¨æˆ·æ€ç»´æ¨¡å¼æ— éœ€è°ƒæ•´

### æŠ€æœ¯åˆ›æ–°ä»·å€¼

Soperatoræœ€å¤§çš„æŠ€æœ¯åˆ›æ–°åœ¨äº**æˆåŠŸå°†ä¼ ç»ŸHPCè°ƒåº¦ç³»ç»Ÿå®¹å™¨åŒ–ï¼ŒåŒæ—¶å®Œå…¨ä¿æŒäº†ç”¨æˆ·ä½“éªŒ**ã€‚è¿™æ„å‘³ç€ä¼ ç»ŸHPCç”¨æˆ·èƒ½å¤Ÿæ— ç¼äº«å—äº‘åŸç”ŸæŠ€æœ¯å¸¦æ¥çš„å¥½å¤„ï¼š

- **å¼¹æ€§èƒ½åŠ›**: è·å¾—äº‘åŸç”Ÿçš„æ‰©ç¼©å®¹èƒ½åŠ›
- **å¯é æ€§**: å…·å¤‡å®¹å™¨åŒ–ç¯å¢ƒçš„è‡ªæ„ˆèƒ½åŠ›
- **è¿ç»´æ•ˆç‡**: äº«å—K8sçš„è‡ªåŠ¨åŒ–è¿ç»´èƒ½åŠ›
- **èµ„æºåˆ©ç”¨ç‡**: åŠ¨æ€è°ƒåº¦æå‡èµ„æºä½¿ç”¨æ•ˆç‡

è¿™ç§æ¶æ„è®©ä¼ ç»ŸHPCç”¨æˆ·èƒ½å¤Ÿæ— ç¼äº«å—äº‘åŸç”ŸæŠ€æœ¯å¸¦æ¥çš„å¥½å¤„ï¼Œæ˜¯HPCå®¹å™¨åŒ–é¢†åŸŸçš„ä¸€ä¸ªé‡è¦çªç ´ã€‚å®ƒè¯æ˜äº†å¤æ‚çš„ä¼ ç»Ÿç³»ç»Ÿå¯ä»¥åœ¨ä¸ç‰ºç‰²ç”¨æˆ·ä½“éªŒçš„å‰æä¸‹ï¼Œå®Œå…¨æ‹¥æŠ±äº‘åŸç”ŸæŠ€æœ¯æ ˆã€‚

---

*æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æäº†Soperatorå¦‚ä½•å®ç°ç”¨æˆ·é€æ˜æœºåˆ¶ï¼Œä¸ºç†è§£ä¼ ç»ŸHPCç³»ç»Ÿäº‘åŸç”ŸåŒ–çš„æŠ€æœ¯å®ç°æä¾›äº†å®Œæ•´çš„å‚è€ƒã€‚*